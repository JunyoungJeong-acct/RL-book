{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CME 241 (Winter 2025) - Assignment 1\n",
    "\n",
    "**Due: Sunday, January 19 @ 11:59 PM PST on Gradescope.**\n",
    "\n",
    "Assignment instructions:\n",
    "- Make sure each of the subquestions have answers\n",
    "- Ensure that group members indicate which problems they're in charge of\n",
    "- Show work and walk through your thought process where applicable\n",
    "- Empty code blocks are for your use, so feel free to create more under each section as needed\n",
    "- Document code with light comments (i.e. 'this function handles visualization')\n",
    "\n",
    "Submission instructions:\n",
    "- When complete, fill out your publicly available GitHub repo file URL and group members below, then export or print this .ipynb file to PDF and upload the PDF to Gradescope.\n",
    "\n",
    "*Link to this ipynb file in your public GitHub repo (replace below URL with yours):* \n",
    "\n",
    "https://github.com/JunyoungJeong-acct/RL-book/blob/master/assignments/assignment1.ipynb\n",
    "\n",
    "*Group members (replace below names with people in your group):* \n",
    "- Junyoung Jeong\n",
    "- Elliot Porter\n",
    "- Andrew Sung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import abc\n",
    "from dataclasses import dataclass\n",
    "from typing import Mapping, Dict\n",
    "from rl.distribution import Categorical, FiniteDistribution\n",
    "from rl.markov_process import FiniteMarkovProcess "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Snakes and Ladders (Led by Andrew)\n",
    "\n",
    "In the classic childhood game of Snakes and Ladders, all players start to the left of square 1 (call this position 0) and roll a 6-sided die to represent the number of squares they can move forward. The goal is to reach square 100 as quickly as possible. Landing on the bottom rung of a ladder allows for an automatic free-pass to climb, e.g. square 4 sends you directly to 14; whereas landing on a snake's head forces one to slide all the way to the tail, e.g. square 34 sends you to 6. Note, this game can be viewed as a Markov Process, where the outcome is only depedent on the current state and not the prior trajectory. In this question, we will ask you to both formally describe the Markov Process that describes this game, followed by coding up a version of the game to get familiar with the RL-book libraries.\n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "How can we model this problem with a Markov Process?\n",
    "\n",
    "---\n",
    "\n",
    "### Subquestions\n",
    "\n",
    "#### Part (A): MDP Modeling\n",
    "\n",
    "Formalize the state space of the Snakes and Ladders game. Don't forget to specify the terminal state!\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (B): Transition Probabilities\n",
    "\n",
    "Write out the structure of the transition probabilities. Feel free to abbreviate all squares that do not have a snake or ladder.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (C): Modeling the Game\n",
    "\n",
    "Code up a `transition_map: Transition[S]` data structure to represent the transition probabilities of the Snakes and Ladders Markov Process so you can model the game as an instance of `FiniteMarkovProcess`. Use the `traces` method to create sampling traces, and plot the graph of the distribution of time steps to finish the game. Use the image below for the locations of the snakes and ladders.\n",
    "\n",
    "![Snakes and Laddders](./Figures/snakesAndLadders.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "\n",
    "\n",
    "Answer:\n",
    "\n",
    "Suppose that there are $n$ players playing the game. The state space of this game $\\mathcal{S} = \\{S \\in \\mathbb{N}^n\\}$ consists of a $n \\times 1$ vector with entries representing position $p_i$ for each player $i \\in \\{1,\\cdots, n\\}$. I assume that the game ends when one of the players finishes the game, so a terminal state is whenever at least one one of the players reaches position 100 - corresponding to a state vector with at least 1 entry of 100. Furthermore, I assume that all players essentially play simultaneously - essentially they all roll the dice at the same time and if multiple players win in the same round they are all winners. \n",
    "\n",
    "In math, the set of terminal states $\\mathcal{T} = \\{S \\in \\mathcal{S} \\mid \\{S\\} \\cap \\{100\\} \\neq \\emptyset\\}$ - essentially is the position of at least one player 100. However, if we wanted to track the ranking of all players and not just the winner, we could let state 100 be an absorbing state, and add additional entries to our vector to track the ranking of the players. \n",
    "\n",
    "Therefore, a sufficient set of positions that comprise the entries of our state vector would be $\\mathbb{N} \\cap [0,100]$.  However, some integers are not positions because they immediately transport the player to another positions, so we can remove them without loss. Specifically, using the image provided in Part (C):\n",
    "\\begin{align}\n",
    "    \\begin{array}{|c|c|}\n",
    "        \\hline\n",
    "        \\text{Removed State}:\\text{Mapping},\n",
    "        \\hline\n",
    "        1:38,\n",
    "        4:14,\n",
    "        8:30,\n",
    "        21:42,\n",
    "        28:78,\n",
    "        32:10,\n",
    "        36:6,\n",
    "        48:26,\n",
    "        50:67,\n",
    "        62:18,\n",
    "        71:92,\n",
    "        80:99,\n",
    "        88:24,\n",
    "        95:56,\n",
    "        97:78,\n",
    "        \\hline\n",
    "    \\end{array}\n",
    "\\end{align}\n",
    "The states in the first column of the table above will be removed from the state space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "I assume that rolling the die to determine the change in position of a player corresponds to a draw from $Unif[\\{1,2,3,4,5,6\\}]$, and that each player's draw is independent of the other players. Let us define a function of the order of positions as $\\sigma(j)$. For example, $\\sigma(1) = 38$ since there is a ladder on position 1 leading to position 38. This notation allows us to concisely to define the structure of transition policies as $\\Pr(S' = \\sigma(S+i)) = 6^{-n}$ for any $i \\in \\{1,2,3,4,5,6\\}^n$. Essentially, we draw $n$ draws from the uniform, add it to every players current position and each unique (maintaining order) change in position will have a chance of occuring with probability $6^{-n}$. Note that this is different from change in state, since we may need to add together repeat states from the snakes and ladders aspect of this game. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I coded the game for 2 players\n",
    "# I am not quite sure how to do a variable number of players as the states class does not take lists\n",
    "\n",
    "# Define the state class for snakes and ladders\n",
    "@dataclass(frozen=True)\n",
    "class SLState:\n",
    "    position1: int\n",
    "    position2: int\n",
    "\n",
    "    def state1(self) -> int:\n",
    "        return position1\n",
    "    def state2(self) -> int:\n",
    "        return position2\n",
    "\n",
    "class SLFiniteMP(FiniteMarkovProcess[SLState]):\n",
    "\n",
    "    def __init__(\n",
    "        self\n",
    "    ):\n",
    "        super().__init__(self.get_transition_map())\n",
    "\n",
    "    def get_transition_map(self) -> \\\n",
    "            Mapping[SLState, FiniteDistribution[SLState]]:\n",
    "        d: Dict[SLState, Categorical[SLState]] = {}\n",
    "        # Dictionary mapping order on board -> position of player\n",
    "        SL_dict = {1:38, 4:14, 8:30, 21:42, 28:78, 32:10, 36:6, 48:26, \n",
    "                    50:67, 62:18, 71:92, 80:99, 88:24, 95:56, 97:78}\n",
    "        positions = np.zeros(2)\n",
    "        # For every player, every position, and every realization of the die\n",
    "        for p1 in range(100):\n",
    "            for p2 in range(100):\n",
    "                state_probs_map: Mapping[SLState, float] = {\n",
    "                    SLState(SL_dict.get(p1 + i + 1,p1 + i + 1),SL_dict.get(p2 + j + 1,p2 + j + 1)): \n",
    "                    1/36 if (p1 and p2) < 100 \n",
    "                    else 0\n",
    "                    for i in range(5)\n",
    "                    for j in range(5)\n",
    "                }\n",
    "                d[SLState(p1,p2)] = Categorical(state_probs_map)\n",
    "        return d\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     n = 2\n",
    "\n",
    "#     Snakes = SLFiniteMP()\n",
    "\n",
    "#     print(\"Transition Map\")\n",
    "#     print(\"--------------\")\n",
    "#     print(Snakes)\n",
    "\n",
    "#     print(\"Stationary Distribution\")\n",
    "#     print(\"-----------------------\")\n",
    "#     Snakes.display_stationary_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Markov Decision Processes (Led by Junyoung)\n",
    "\n",
    "Consider an MDP with an infinite set of states $\\mathcal{S} = \\{1,2,3,\\ldots \\}$. The start state is $s=1$. Each state $s$ allows a continuous set of actions $a \\in [0,1]$. The transition probabilities are given by: \n",
    "$$\\mathbb{P}[s+1 \\mid s, a] = a, \\mathbb{P}[s \\mid s, a] = 1 - a \\text{ for all } s \\in \\mathcal{S} \\text{ for all } a \\in [0,1]$$\n",
    "For all states $s \\in \\mathcal{S}$ and actions $a \\in [0,1]$, transitioning from $s$ to $s+1$ results in a reward of $1-a$ and transitioning from $s$ to $s$ results in a reward of $1+a$. The discount factor $\\gamma=0.5$.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "How can we derive a mathematical formulation for the value function and the optimal policy? And how do those functions change when we modify the action space?\n",
    "\n",
    "The decision maker (agent) solves the following problem.\n",
    "\\begin{align}\n",
    "    V^\\pi(s) = R(s_0,a_0) + \\gamma \\mathbb{E}_0\\left[\\sum\\limits_{1}^\\infty \\sum\\limits_{1}^\\infty \\pi(s_t,a_t) R(s_t,a_t)\\right]\n",
    "\\end{align}\n",
    "\n",
    "---\n",
    "\n",
    "### Subquestions\n",
    "\n",
    "#### Part (A): Optimal Value Function  \n",
    "\n",
    "Using the MDP Bellman Optimality Equation, calculate the Optimal Value Function $V^*(s)$ for all $s \\in \\mathcal{S}$. Given $V^*(s)$, what is the optimal action, $a^*$, that maximizes the optimal value function?\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (B): Optimal Policy  \n",
    "\n",
    "Calculate an Optimal Deterministic Policy $\\pi^*(s)$ for all $s \\in \\mathcal{S}$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (C): Changing the Action Space  \n",
    "\n",
    "Let's assume that we modify the action space such that instead of $a \\in [0,1]$ for all states, we restrict the action space to $a \\in \\left[0,\\frac{1}{s}\\right]$ for state $s$. This means that higher states have more restricted action spaces. How does this constraint affect:\n",
    "\n",
    "- The form of the Bellman optimality equation?\n",
    "- The optimal value function, $V^*(s)$?\n",
    "- The structure of the optimal policy, $\\pi^*(s)$?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "The optimal value function $V^*(s)$ is as follows.\n",
    "\\begin{align}\n",
    "    V^* = \\max_{a \\in[0,1]} 0.5 \\mathbb{E}_{a}\\left[ \\pi(s,a) \\left(a [(1-a) + V^*] + (1-a) [(1+a) + V^*] \\right)\\right]\n",
    "\\end{align}\n",
    "Since only the change in state and not the realization of the state affects the reward function, we can drop the dependency on $s$ in our value function.\n",
    "\n",
    "Maximizing $a [(1-a) + V^*(s+1)] + (1-a) [(1+a) + V^*(s)]$ yields $a = \\frac{1}{4}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "The deterministic policy is $\\pi^*(s) = \\pi(s, \\frac{1}{4}) = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer\n",
    "\n",
    "#### Bellman Optimality Equation Change:\n",
    "The agent's Bellman Optimality equation is not changed much, but we can no longer drop the dependence on the state in $V^*$ as before.\n",
    "\\begin{align}\n",
    "    0.5 \\mathbb{E}_{a}\\left[ \\pi(s,a) \\left(a [(1-a) + V^*(s+1)] + (1-a) [(1+a) + V^*(s)] \\right)\\right]\n",
    "\\end{align}\n",
    "\n",
    "#### Optimal Value Function Change:\n",
    "The optimal value function $V^*(s)$ is now as follows, with the key difference in the restriction on the value of $a$ in the max operator.\n",
    "\\begin{align}\n",
    "   V^*(s) = \\max_{a \\in [0, \\frac{1}{s}]} 0.5 \\mathbb{E}_{a}\\left[ \\pi(s,a) \\left(a [(1-a) + V^*(s+1)] + (1-a) [(1+a) + V^*(s)] \\right)\\right]\n",
    "\\end{align}\n",
    "\n",
    "#### Optimal Policy Change:\n",
    "The structure of the optimal policy has changed as it is now to choose $\\max\\{\\frac{1}{4},\\frac{1}{s}\\}$. We know this to be the case since $\\left(a [(1-a) + V^*(s+1)] + (1-a) [(1+a) + V^*(s)] \\right)$ is strictly increasing for $a < \\frac{1}{4}$, so the agent will choose the highest possible $a$ if restricted to an action lower than $\\frac{1}{4}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Frog in a Pond (Led by ______)\n",
    "\n",
    "Consider an array of $n+1$ lilypads on a pond, numbered $0$ to $n$. A frog sits on a lilypad other than the lilypads numbered $0$ or $n$. When on lilypad $i$ ($1 \\leq i \\leq n-1$), the frog can croak one of two sounds: **A** or **B**. \n",
    "\n",
    "- If it croaks **A** when on lilypad $i$ ($1 \\leq i \\leq n-1$):\n",
    "  - It is thrown to lilypad $i-1$ with probability $\\frac{i}{n}$.\n",
    "  - It is thrown to lilypad $i+1$ with probability $\\frac{n-i}{n}$.\n",
    "  \n",
    "- If it croaks **B** when on lilypad $i$ ($1 \\leq i \\leq n-1$):\n",
    "  - It is thrown to one of the lilypads $0, \\ldots, i-1, i+1, \\ldots, n$ with uniform probability $\\frac{1}{n}$.\n",
    "\n",
    "A snake, perched on lilypad $0$, will eat the frog if it lands on lilypad $0$. The frog can escape the pond (and hence, escape the snake!) if it lands on lilypad $n$.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "What should the frog croak when on each of the lilypads $1, 2, \\ldots, n-1$, in order to maximize the probability of escaping the pond (i.e., reaching lilypad $n$ before reaching lilypad $0$)? \n",
    "\n",
    "Although there are multiple ways to solve this problem, we aim to solve it by modeling it as a **Markov Decision Process (MDP)** and identifying the **Optimal Policy**.\n",
    "\n",
    "---\n",
    "\n",
    "### Subquestions\n",
    "\n",
    "#### Part (A): MDP Modeling\n",
    "\n",
    "Express the frog-escape problem as an MDP using clear mathematical notation by defining the following components: \n",
    "\n",
    "- **State Space**: Define the possible states of the MDP. \n",
    "- **Action Space**: Specify the actions available to the frog at each state. \n",
    "- **Transition Function**: Describe the probabilities of transitioning between states for each action. \n",
    "- **Reward Function**: Specify the reward associated with the states and transitions. \n",
    "\n",
    "---\n",
    "\n",
    "#### Part (B): Python Implementation\n",
    "\n",
    "There is starter code below to solve this problem programatically. Fill in each of the $6$ `TODO` areas in the code. As a reference for the transition probabilities and rewards, you can make use of the example in slide 16/31 from the following slide deck: https://github.com/coverdrive/technical-documents/blob/master/finance/cme241/Tour-MP.pdf.\n",
    "\n",
    "Write Python code that:\n",
    "\n",
    "- Models this MDP.\n",
    "- Solves the **Optimal Value Function** and the **Optimal Policy**.\n",
    "\n",
    "Feel free to use/adapt code from the textbook. Note, there are other libraries that are needed to actually run this code, so running it will not do anything. Just fill in the code so that it could run assuming that the other libraries are present.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (C): Visualization and Analysis\n",
    "\n",
    "After running the code, we observe the following graphs for $n=3$, $n=10$, and $n=25$:\n",
    "\n",
    "![FrogGraphs](./Figures/frogGraphs.png)\n",
    "\n",
    "What patterns do you observe for the **Optimal Policy** as you vary $n$ from $3$ to $25$? When the frog is on lilypad $13$ (with $25$ total), what action should the frog take? Is this action different than the action the frog should take if it is on lilypad $1$?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "#### State Space:  \n",
    "\n",
    "The state space is $\\mathcal{S} = \\{i \\in\\mathbb{N} | 0 \\leq i \\leq n\\}$. The set of terminal states is $\\mathcal{T} = \\{0, n\\}$.\n",
    "\n",
    "#### Action Space:  \n",
    "\n",
    "The frog's actions set is $\\mathcal{A} = \\{A,B\\}$.\n",
    "\n",
    "#### Transition Function:  \n",
    "\n",
    "The transition function $\\pi\\mathcal{S} \\times \\mathcal{A} \\to [0,1]$ can be defined as:\n",
    "\\begin{align}\n",
    "    \\pi(i,a,i') &= \n",
    "    \\begin{cases}\n",
    "        0 & \\text{if $i \\in \\mathcal{T}$}\\\\\n",
    "        \\frac{i}{n} & \\text{if $i \\notin \\mathcal{T}$ and $i' = i-1$ and a = A}\\\\\n",
    "        \\frac{n-i}{n} & \\text{if $i \\notin \\mathcal{T}$ and $i' = i+1$ and a = A}\\\\ \n",
    "        \\frac{1}{n} & \\text{if $i \\notin \\mathcal{T}$ and a = B}\\\\\n",
    "    \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "#### Reward Function:  \n",
    "\n",
    "The reward function $\\mathcal{R}:\\mathcal{S} \\to \\mathbb{\\bar{R}}$ is defined\n",
    "\\begin{align}\n",
    "    R(i) &= \n",
    "    \\begin{cases}\n",
    "        \\infty & \\text{if $i = n$}\\\\\n",
    "        -\\infty & \\text{if $i = 0$}\n",
    "    \\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B' 'A' 'A' 'A' 'A' 'A' 'A' 'A' 'A' 'A' 'A' 'A' 'A' 'A' 'A' 'A' 'A' 'A'\n",
      " 'A' 'A' 'A' 'A' 'A' 'A']\n"
     ]
    }
   ],
   "source": [
    "MDPRefined = dict\n",
    "def get_lily_pads_mdp(n: int) -> MDPRefined:\n",
    "    data = {\n",
    "        i: {\n",
    "            'A': {\n",
    "                i - 1: i/n, # TODO: fill in with the correct transition probabilities\n",
    "                i + 1: (n-i)/n, # TODO: fill in with the correct transition probabilities\n",
    "            },\n",
    "            'B': {\n",
    "                i: 1/n, # TODO: fill in with the correct transition probabilities\n",
    "            }\n",
    "        } for i in range(1, n)\n",
    "    }\n",
    "    data[0] = 0 # TODO: this is the initial state, so what would be the correct transition probabilities?\n",
    "    data[n] = 0 # TODO: similarly, this is the terminal state, so what would be the correct transition probabilities?\n",
    "    return MDPRefined(data)\n",
    "\n",
    "# This will give us our optimal value function by taking the fixed point of the Bellman\n",
    "Mapping = dict\n",
    "def direct_bellman(n: int) -> Mapping[int, float]:\n",
    "    probs = get_lily_pads_mdp(n)\n",
    "    vf = [0.5] * (n + 1)\n",
    "    vf[0] = -1000000000\n",
    "    vf[n] = 1000000000\n",
    "    tol = 1e-8\n",
    "    epsilon = tol * 1e4\n",
    "    while epsilon >= tol:\n",
    "        old_vf = [v for v in vf]\n",
    "        for i in range(1, n):\n",
    "            vf_A = probs[i][\"A\"][i-1]*old_vf[i-1] + probs[i][\"A\"][i+1]*old_vf[i+1]\n",
    "            vf_B = 0\n",
    "            for b in range(0,n):\n",
    "                vf_B = vf_B + probs[i][\"B\"][i]*old_vf[b]\n",
    "            vf[i] = 0.5*(max(vf_A,vf_B)) # TODO: fill in with the Bellman update\n",
    "        epsilon = max(abs(old_vf[i] - v) for i, v in enumerate(vf))\n",
    "    return {v: f for v, f in enumerate(vf)}\n",
    "\n",
    "# We can recover the optimal policy as follows (for n = 25)\n",
    "n = 25\n",
    "optimal_policy = np.repeat(\"C\", n-1)\n",
    "vStar = direct_bellman(n) \n",
    "probs = get_lily_pads_mdp(n)\n",
    "for i in range(1, n):\n",
    "    vf_A = probs[i][\"A\"][i-1]*vStar[i-1] + probs[i][\"A\"][i+1]*vStar[i+1]\n",
    "    vf_B = 0\n",
    "    for b in range(0,n): \n",
    "        vf_B = vf_B + probs[i][\"B\"][i]*vStar[b] \n",
    "    if vf_A >= vf_B:\n",
    "        optimal_policy[i-1] = \"A\"\n",
    "    else:\n",
    "        optimal_policy[i-1] = \"B\"\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer\n",
    "\n",
    "The optimal policy is represented by the action with a higher value for $Q^*$. In all 3 parameterizations, the optimal policy for the frog is to choose $A$ for every lilypad $>1$. Therefore the frog's optimal action on lilypad 13 is $A$, but when the frog is on lilypad $1$ the frog should choose $B$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Manual Value Iteration (Led by Elliot Porter)\n",
    "\n",
    "Consider a simple MDP with $\\mathcal{S} = \\{s_1, s_2, s_3\\}, \\mathcal{T} = \\{s_3\\}, \\mathcal{A} = \\{a_1, a_2\\}$. The State Transition Probability function  \n",
    "$$\\mathcal{P}: \\mathcal{N} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0, 1]$$  \n",
    "is defined as:  \n",
    "$$\\mathcal{P}(s_1, a_1, s_1) = 0.25, \\mathcal{P}(s_1, a_1, s_2) = 0.65, \\mathcal{P}(s_1, a_1, s_3) = 0.1$$  \n",
    "$$\\mathcal{P}(s_1, a_2, s_1) = 0.1, \\mathcal{P}(s_1, a_2, s_2) = 0.4, \\mathcal{P}(s_1, a_2, s_3) = 0.5$$  \n",
    "$$\\mathcal{P}(s_2, a_1, s_1) = 0.3, \\mathcal{P}(s_2, a_1, s_2) = 0.15, \\mathcal{P}(s_2, a_1, s_3) = 0.55$$  \n",
    "$$\\mathcal{P}(s_2, a_2, s_1) = 0.25, \\mathcal{P}(s_2, a_2, s_2) = 0.55, \\mathcal{P}(s_2, a_2, s_3) = 0.2$$  \n",
    "\n",
    "The Reward Function  \n",
    "$$\\mathcal{R}: \\mathcal{N} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$$  \n",
    "is defined as:  \n",
    "$$\\mathcal{R}(s_1, a_1) = 8.0, \\mathcal{R}(s_1, a_2) = 10.0$$  \n",
    "$$\\mathcal{R}(s_2, a_1) = 1.0, \\mathcal{R}(s_2, a_2) = -1.0$$  \n",
    "\n",
    "Assume a discount factor of $\\gamma = 1$.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Your task is to determine an Optimal Deterministic Policy **by manually working out** (not with code) the first two iterations of the Value Iteration algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "### Subquestions\n",
    "\n",
    "#### Part (A): 2 Iterations\n",
    "\n",
    "1. Initialize the Value Function for each state to be its $\\max$ (over actions) reward, i.e., we initialize the Value Function to be $v_0(s_1) = 10.0, v_0(s_2) = 1.0, v_0(s_3) = 0.0$. Then manually calculate $q_k(\\cdot, \\cdot)$ and $v_k(\\cdot)$ from $v_{k - 1}(\\cdot)$ using the Value Iteration update, and then calculate the greedy policy $\\pi_k(\\cdot)$ from $q_k(\\cdot, \\cdot)$ for $k = 1$ and $k = 2$ (hence, 2 iterations).\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (B): Argument\n",
    "\n",
    "1. Now argue that $\\pi_k(\\cdot)$ for $k > 2$ will be the same as $\\pi_2(\\cdot)$. *Hint*: You can make the argument by examining the structure of how you get $q_k(\\cdot, \\cdot)$ from $v_{k-1}(\\cdot)$. With this argument, there is no need to go beyond the two iterations you performed above, and so you can establish $\\pi_2(\\cdot)$ as an Optimal Deterministic Policy for this MDP.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (C): Policy Evaluation\n",
    "\n",
    "1. Using the policy $\\pi_2(\\cdot)$, compute the exact value function $V^{\\pi_2}(s)$ for all $s\\in S$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (D): Sensitivity Analysis\n",
    "\n",
    "Assume the reward for $R(s_1, a_2)$ is modified to $11.0$ instead of $10.0$.\n",
    "\n",
    "1. Perform one iteration of Value Iteration starting from the initialized value function $v_0(s)$, where $v_0(s)$ remains the same as in the original problem.\n",
    "2. Determine whether this change impacts the Optimal Deterministic Policy $\\pi(\\cdot)$. If it does, explain why.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "Our starting guess for the value function is:\n",
    "\\begin{align}\n",
    "    v_0(s_1) = 10.0\\\\\n",
    "    v_0(s_2) = 1.0\\\\\n",
    "    v_0(s_3) = 0.0\n",
    "\\end{align}\n",
    "Our Bellman Operator $\\mathcal{B}$ is:\n",
    "\\begin{align}\n",
    "   v_{k} = \\mathcal{B}(v_{k-1}) = r(s,a) + \\sum_{s'} \\mathcal{P}(s,a,s') v_{k-1}(s')\n",
    "\\end{align}\n",
    "Therefore the action value functions are as follows.\n",
    "\\begin{align}\n",
    "    q_1(s_1,a_1) = 8.0 + 0.25 * 10 + 0.65 * 1 + 0.1 * 0 = 11.5 \\\\\n",
    "    q_1(s_2,a_1) = 1.0 + 0.1 * 10 + 0.4 * 1 + 0.5 * 0 = 2.4\\\\\n",
    "    q_1(s_1,a_2) = 10.0 + 0.3 * 10 + 0.15 * 1 + 0.55 * 0 = 13.15\\\\\n",
    "    q_1(s_2,a_2) = -1.0 + 0.25 * 10 + 0.55 * 1 + 0.2 * 0 = 2.05\n",
    "\\end{align}\n",
    "Applying a greedy policy yields the following state value functions.\n",
    "\\begin{align}\n",
    "    v_1(s_2) = 13.5\\\\\n",
    "    v_1(s_2) = 2.4\\\\\n",
    "    v_1(s_3) = 0.0\n",
    "\\end{align}\n",
    "Repeat one more round. Action value functions.\n",
    "\\begin{align}\n",
    "    q_2(s_1,a_1) = 8.0 + 0.25 * 13.5 + 0.65 * 2.4 + 0.1 * 0 = 12.935 \\\\\n",
    "    q_2(s_2,a_1) = 1.0 + 0.1 * 13.5 + 0.4 * 2.4 + 0.5 * 0 = 3.31\\\\\n",
    "    q_2(s_1,a_2) = 10.0 + 0.3 * 13.5 + 0.15 * 2.4 + 0.55 * 0 = 14.41\\\\\n",
    "    q_2(s_2,a_2) = -1.0 + 0.25 * 13.5 + 0.55 * 2.4 + 0.2 * 0 = 3.695\n",
    "\\end{align}\n",
    "Applying a greedy policy yields the following state value functions.\n",
    "\\begin{align}\n",
    "    v_2(s_2) = 14.41\\\\\n",
    "    v_2(s_2) = 3.695\\\\\n",
    "    v_2(s_3) = 0.0\n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer:  \n",
    "\n",
    "In iteration 2 of our fixed point algorithm - the optimal deterministic policy is to choose action $a_2$ in both states $s_1$ and $s_2$. This is because the action value function is higher in both states under action 2. This will be the optimal policy for any iteration $k$ because the only way that $a_1$ will be preferred is if the value function in state 1 or 2 decreases, or if the value in the terminal state somehow becomes nonzero. However, the weighted average of the values in state 1 and 2 must be weakly increasing over iterations of our algorithm, meaning that action 2 will be optimal in perpetuity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer:  \n",
    "\n",
    "Given that choosing $a_2$ is always optimal - we can solve for our optimal values as follows.\n",
    "\\begin{align}\n",
    "    V^{\\pi^2}(s_1) = 10.0 + 0.3 * V^{\\pi^2}(s_1) + 0.15 * V^{\\pi^2}(s_2)\\\\\n",
    "    V^{\\pi^2}(s_2) = -1.0 + 0.25 * V^{\\pi^2}(s_1) + 0.55 * V^{\\pi^2}(s_2)\\\\\n",
    "    V^{\\pi^2}(s_3) = 0\n",
    "\\end{align}\n",
    "Solving this system yields \n",
    "\\begin{align}\n",
    "    V^{\\pi^2}(s_1) = 15.68\\\\\n",
    "    V^{\\pi^2}(s_2) = 6.49\\\\\n",
    "    V^{\\pi^2}(s_3) = 0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (D) Answer\n",
    "\n",
    "#### Value Iteration:  \n",
    "\n",
    "Our starting guess for the value function is now\n",
    "\\begin{align}\n",
    "    v_0(s_1) = 11.0\\\\\n",
    "    v_0(s_2) = 1.0\\\\\n",
    "    v_0(s_3) = 0.0\n",
    "\\end{align}\n",
    "Our Bellman Operator $\\mathcal{B}$ is:\n",
    "\\begin{align}\n",
    "   v_{k} = \\mathcal{B}(v_{k-1}) = r(s,a) + \\sum_{s'} \\mathcal{P}(s,a,s') v_{k-1}(s')\n",
    "\\end{align}\n",
    "Therefore the action value functions are as follows.\n",
    "\\begin{align}\n",
    "    q_1(s_1,a_1) = 8.0 + 0.25 * 11 + 0.65 * 1 + 0.1 * 0 = 11.75 \\\\\n",
    "    q_1(s_2,a_1) = 1.0 + 0.1 * 11+ 0.4 * 1 + 0.5 * 0 = 2.5\\\\\n",
    "    q_1(s_1,a_2) = 11 + 0.3 * 11 + 0.15 * 1 + 0.55 * 0 = 14.45\\\\\n",
    "    q_1(s_2,a_2) = -1.0 + 0.25 * 11 + 0.55 * 1 + 0.2 * 0 = 2.3\n",
    "\\end{align}\n",
    "Applying a greedy policy yields the following state value functions.\n",
    "\\begin{align}\n",
    "    v_1(s_2) = 13.5\\\\\n",
    "    v_1(s_2) = 2.4\\\\\n",
    "    v_1(s_3) = 0.0\n",
    "\\end{align}\n",
    "\n",
    "#### Optimal Deterministic Policy:  \n",
    "\n",
    "The optimal deterministic policy remains unchanged - always pick $a_2$. The gap between $q_1(s_2,a_1)$ and $q_1(s_2,a_2)$ is even smaller than in the previous parameter setup so in the second iteration of our algorithm we will have the same result emerge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Fixed-Point and Policy Evaluation True/False Questions (Led by Andrew Sung)\n",
    "\n",
    "### Recall Section: Key Formulas and Definitions\n",
    "\n",
    "#### Bellman Optimality Equation\n",
    "The Bellman Optimality Equation for state-value functions is:\n",
    "$$\n",
    "V^*(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s, a, s') V^*(s') \\right].\n",
    "$$\n",
    "For action-value functions:\n",
    "$$\n",
    "Q^*(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s, a, s') \\max_{a'} Q^*(s', a').\n",
    "$$\n",
    "\n",
    "#### Contraction Property\n",
    "The Bellman Policy Operator $B^\\pi$ is a contraction under the $L^\\infty$-norm:\n",
    "$$\n",
    "\\|B^\\pi(X) - B^\\pi(Y)\\|_\\infty \\leq \\gamma \\|X - Y\\|_\\infty.\n",
    "$$\n",
    "This guarantees convergence to a unique fixed point.\n",
    "\n",
    "#### Policy Iteration\n",
    "Policy Iteration alternates between:\n",
    "1. **Policy Evaluation**: Compute $V^\\pi$ for the current policy $\\pi$.\n",
    "2. **Policy Improvement**: Generate a new policy $\\pi'$ by setting:\n",
    "   $$\n",
    "   \\pi'(s) = \\arg\\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s, a, s') V^\\pi(s') \\right].\n",
    "   $$\n",
    "\n",
    "#### Discounted Return\n",
    "The discounted return from time step $t$ is:\n",
    "$$\n",
    "G_t = \\sum_{i=t+1}^\\infty \\gamma^{i-t-1} R_i,\n",
    "$$\n",
    "where $\\gamma \\in [0, 1)$ is the discount factor.\n",
    "\n",
    "### True/False Questions (Provide Justification)\n",
    "\n",
    "1. **True/False**: If $Q^\\pi(s, a) = 5$, $P(s, a, s') = 0.5$ for $s' \\in \\{s_1, s_2\\}$, and the immediate reward $R(s, a)$ increases by $2$, the updated action-value function $Q^\\pi(s, a)$ also increases by $2$.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "2. **True/False**: For a discount factor $\\gamma = 0.9$, the discounted return for rewards $R_1 = 5, R_2 = 3, R_3 = 1$ is greater than $6$.\n",
    "\n",
    "---\n",
    "\n",
    "3. **True/False**: The Bellman Policy Operator $B^\\pi(V) = R^\\pi + \\gamma P^\\pi \\cdot V$ satisfies the contraction property for all $\\gamma \\in [0, 1)$, ensuring a unique fixed point.\n",
    "\n",
    "---\n",
    "\n",
    "4. **True/False**: In Policy Iteration, the Policy Improvement step guarantees that the updated policy $\\pi'$ will always perform strictly better than the previous policy $\\pi$.\n",
    "\n",
    "---\n",
    "\n",
    "5. **True/False**: If $Q^\\pi(s, a) = 10$ for all actions $a$ in a state $s$, then the corresponding state-value function $V^\\pi(s) = 10$, regardless of the policy $\\pi$.\n",
    "\n",
    "---\n",
    "\n",
    "6. **True/False**: The discounted return $G_t = \\sum_{i=t+1}^\\infty \\gamma^{i-t-1} R_i$ converges to a finite value for any sequence of bounded rewards if $\\gamma < 1$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers (Provide justification, brief explanations are fine)\n",
    "\n",
    "#### Question 1:  \n",
    "\n",
    "**Answer**: TRUE\n",
    "\n",
    "**Justification**:  \n",
    "By definition of the action-value function under a policy $\\pi$:\n",
    "$$\n",
    "Q^\\pi(s,a) \\;=\\; R(s,a) \\;+\\; \\gamma \\sum_{s'} P(s,a,s') \\, V^\\pi(s').\n",
    "$$\n",
    "Increasing $R(s,a)$ by 2 adds exactly $+2$ to $Q^\\pi(s,a)$ because the immediate reward term appears *without* a discount factor (i.e., multiplied by $\\gamma^0 = 1$). Therefore, $Q^\\pi(s,a)$ also increases by 2.\n",
    "\n",
    "However, we note that if $R(s,a)$ is effected for future rewards as well - then the action value may increase more than 2 rendering this statement false. \n",
    "\n",
    "\n",
    "#### Question 2:  \n",
    "\n",
    "**Answer**: TRUE \n",
    "\n",
    "**Justification**:  \n",
    "The discounted sum is\n",
    "$$\n",
    "5 + 0.9 \\times 3 + 0.9^2 \\times 1 \n",
    "\\;=\\; 5 + 2.7 + 0.81\n",
    "\\;=\\; 8.51,\n",
    "$$\n",
    "which is clearly more than $6$.\n",
    "\n",
    "#### Question 3:  \n",
    "\n",
    "**Answer**: TRUE\n",
    "\n",
    "**Justification**:  \n",
    "It is a standard result that $B^\\pi$ is a $\\gamma$-contraction in the $L^\\infty$ norm as long as $\\gamma < 1$. By the Banach Fixed-Point Theorem, a $\\gamma$-contraction has exactly one fixed point, which is $V^\\pi$.\n",
    "\n",
    "#### Question 4:  \n",
    "\n",
    "**Answer**: FALSE\n",
    "\n",
    "**Justification**:  \n",
    "The Policy Improvement Theorem states that $\\pi'$ is *at least as good as* $\\pi$. It will be strictly better unless $\\pi$ was already optimal. If $\\pi$ is optimal, then $\\pi' = \\pi$, so there is no strict improvement.\n",
    "\n",
    "#### Question 5:  \n",
    "\n",
    "**Answer**: TRUE\n",
    "\n",
    "**Justification**:  \n",
    "Recall that\n",
    "$$\n",
    "V^\\pi(s) \\;=\\; \\sum_{a} \\pi(a \\mid s) \\, Q^\\pi(s,a).\n",
    "$$\n",
    "If $Q^\\pi(s,a) = 10$ *for every action* $a$, then any weighted average of 10 is still 10, so $V^\\pi(s) = 10$ regardless of how $\\pi$ distributes probability over $a$.\n",
    "\n",
    "\n",
    "\n",
    "#### Question 6:  \n",
    "\n",
    "\n",
    "**Answer**: TRUE\n",
    "\n",
    "**Justification**:  \n",
    "If $\\lvert R_i\\rvert \\le M$ for all $i$, then each term satisfies\n",
    "$$\n",
    "\\bigl|\\gamma^{\\,i - (t+1)} R_i\\bigr| \\,\\le\\, M \\,\\gamma^{\\,i - (t+1)}.\n",
    "$$\n",
    "This is bounded by the convergent geometric series $\\sum_{i=t+1}^\\infty M \\,\\gamma^{\\,i - (t+1)}$. Therefore, for $\\gamma < 1$, $G_t$ converges to a finite value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
