{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CME 241 (Winter 2025) - Assignment 3\n",
    "\n",
    "**Due: Sunday, February 23 @ 11:59 PM PST on Gradescope.**\n",
    "\n",
    "Assignment instructions:\n",
    "- Make sure each of the subquestions have answers\n",
    "- Ensure that group members indicate which problems they're in charge of\n",
    "- Show work and walk through your thought process where applicable\n",
    "- Empty code blocks are for your use, so feel free to create more under each section as needed\n",
    "- Document code with light comments (i.e. 'this function handles visualization')\n",
    "\n",
    "Submission instructions:\n",
    "- When complete, fill out your publicly available GitHub repo file URL and group members below, then export or print this .ipynb file to PDF and upload the PDF to Gradescope.\n",
    "\n",
    "*Link to this ipynb file in your public GitHub repo (replace below URL with yours):* \n",
    "\n",
    "https://github.com/my-username/my-repo/assignment-file-name.ipynb\n",
    "\n",
    "*Group members (replace below names with people in your group):* \n",
    "- Person 1\n",
    "- Person 2\n",
    "- Person 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from scipy.optimize import minimize_scalar\n",
    "from typing import Iterable, Iterator, Mapping, TypeVar, Callable, Sequence, Tuple, Dict\n",
    "from rl.distribution import Categorical\n",
    "from rl.markov_process import FiniteMarkovProcess, FiniteMarkovRewardProcess\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.policy import FiniteDeterministicPolicy\n",
    "from rl.chapter10.prediction_utils import compare_td_and_mc\n",
    "X = TypeVar('X')\n",
    "S = TypeVar('S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Milk Vendor Optimization Problem (Led by Junyoung)\n",
    "\n",
    "You are a milk vendor and your task is to bring to your store a supply (denoted $S \\in \\mathbb{R}$) of milk volume in the morning that will give you the best profits. You know that the demand for milk throughout the course of the day is a probability distribution function $f$ (for mathematical convenience, assume people can buy milk in volumes that are real numbers, hence milk demand $x \\in \\mathbb{R}$ is a continuous variable with a probability density function). \n",
    "\n",
    "For every extra gallon of milk you carry at the end of the day (supply $S$ exceeds random demand $x$), you incur a cost of $h$ (effectively the wasteful purchases amounting to the difference between your purchase price and the end-of-day discount disposal price since you are not allowed to sell the same milk the next day). For every gallon of milk that a customer demands that you don’t carry (random demand $x$ exceeds supply $S$), you incur a cost of $p$ (effectively the missed sales revenue amounting to the difference between your sales price and purchase price). \n",
    "\n",
    "Your task is to identify the optimal supply $S$ that minimizes your **Expected Cost** $g(S)$, given by:\n",
    "\n",
    "$$\n",
    "g_1(S) = \\mathbb{E}[\\max(x - S, 0)] = \\int_{S}^{\\infty} (x - S) \\cdot f(x) \\, dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "g_2(S) = \\mathbb{E}[\\max(S - x, 0)] = \\int_{-\\infty}^{S} (S - x) \\cdot f(x) \\, dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "g(S) = p \\cdot g_1(S) + h \\cdot g_2(S)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Part (A): Analytical Optimization\n",
    "\n",
    "1. **Derive the first-order condition (FOC)** for minimizing the expected cost $g(S)$.\n",
    "2. **Solve the FOC** to express the optimal supply $S^*$ in terms of the given parameters: $p$, $h$, and the demand distribution $f(x)$. (*Hint*: Pay attention to the balance between the costs of overstocking and understocking)\n",
    "\n",
    "3. **Interpretation**: Provide an interpretation of the condition you derived. What does the balance between $p$ and $h$ imply about the optimal supply $S^*$?\n",
    "\n",
    "---\n",
    "\n",
    "### Part (B): Special Case Analysis\n",
    "\n",
    "1. Consider the case where the demand $x$ follows an **exponential distribution** with parameter $\\lambda > 0$. That is, $f(x) = \\lambda e^{-\\lambda x}$ for $x \\geq 0$.\n",
    "    - Derive an explicit expression for the optimal supply $S^*$.\n",
    "    \n",
    "2. Consider the case where the demand $x$ follows a **normal distribution** with mean $\\mu$ and variance $\\sigma^2$, i.e., $x \\sim \\mathcal{N}(\\mu, \\sigma^2)$. \n",
    "    - Set up the integral for $g(S)$ and describe how it relates to the **cumulative distribution function (CDF)** of the normal distribution.\n",
    "    - Provide an interpretation of how changes in $\\mu$ and $\\sigma$ influence the optimal $S^*$. \n",
    "\n",
    "---\n",
    "\n",
    "### Part (C): Framing as a Financial Options Problem\n",
    "\n",
    "1. Frame the milk vendor’s problem as a **portfolio of call and put options**:\n",
    "    - Identify the analog of the “strike price” and “underlying asset.”\n",
    "    - Explain which part of the cost function $g_1(S)$ or $g_2(S)$ corresponds to a call option and which part to a put option.\n",
    "    - What do $p$ and $h$ represent in this options framework?\n",
    "\n",
    "2. Explain how this framing could be used to derive the optimal supply $S^*$ if solved using financial engineering concepts.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (D): Numerical Solution and Simulation\n",
    "\n",
    "1. **Numerical Solution**: Write a Python function that numerically estimates the optimal $S^*$ using an iterative search or numerical optimization method. \n",
    "\n",
    "2. **Simulation**: Generate random samples of milk demand from an exponential distribution and simulate the total costs for different values of $S$. Plot the costs against $S$ and visually identify the optimal $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "1. Apply the Leibniz rule to $g_1(s)$ and $g_2(s)$. To apply this rule, it is necessary to assume that $f(x)$ is continuously differentiable in $x$. Furthermore, I assume that $f(x)$ does not have fat tails.\n",
    "\\begin{align}\n",
    "\\frac{d}{d S}\\left[\\int_{S}^{\\infty} (x - S) \\cdot f(x) \\, dx\\right] \n",
    "&= (\\infty - S) f(\\infty)\\cdot 0 - (S-S) f(S) \\cdot 1 + \\int_{S}^{\\infty} - f(x) \\, dx\\\\\n",
    "&= \\int_{S}^{\\infty} - f(x) \\, dx\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d}{d S}\\left[\\int_{-\\infty}^{S} (S - x) \\cdot f(x) \\, dx\\right ]\n",
    "&= (S-S)f(S)\\cdot 0 - (-\\infty - S) f(-\\infty) \\cdot 1 + \\int_{-\\infty}^{S} f(x) \\, dx\\\\\n",
    "&= \\int_{-\\infty}^{S} f(x) \\, dx\n",
    "\\end{align}\n",
    "\n",
    "Combining these derivatives yields the first order condition for the objective.\n",
    "$$\n",
    "0 = p \\cdot \\int_{S}^{\\infty} - f(x) \\, dx + h \\cdot \\int_{-\\infty}^{S} f(x) \\, dx\n",
    "$$\n",
    "\n",
    "2. Let $F(s) = \\int_{-\\infty}^(s) f(s) \\, ds$. Note that since $F(s)$ is a cumulative distribution function $F(\\infty) = 1$. Substituting this expression into our FOC yields the following.\n",
    "$$\n",
    "0 = p \\cdot (1- F(S)) + h \\cdot F(S)\n",
    "$$\n",
    "Thus the optimal $S$, which we will denote $S^*$ can be characterized as the following.\n",
    "$$\n",
    "S^* = F^{-1}(\\frac{p}{p+h})\n",
    "$$\n",
    "where $F^{-1}(\\cdot)$ is the inverse function of $F(\\cdot)$.\n",
    "\n",
    "3. We can interpret this condition as picking $S$ such that the the probability of having insufficient supply is equal to $\\frac{p}{p+h}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "1. Now suppose that demand $x$ follows an **exponential distribution** with parameter $\\lambda > 0$. This implies $f(x) = \\lambda e^{-\\lambda x}$ for $x \\geq 0$ and $0. for all $x < 0$. Thus we can write $F(x) = 1- e^{-\\lambda x}$. \n",
    "$$\n",
    "S^* = F^{-1}(\\frac{p}{p+h}) = - \\frac{1}{\\lambda}\\ln(1 - \\frac{p}{p+h}) = - \\frac{1}{\\lambda}(\\ln(h) - \\ln(p+h))\n",
    "$$\n",
    "    \n",
    "2. Now suppose that demand $x$ follows a **normal distribution** with mean $\\mu$ and variance $\\sigma^2$, i.e., $x \\sim \\mathcal{N}(\\mu, \\sigma^2)$. \n",
    "\n",
    "    - We can write $g(S)$ in terms of the cumulative distribution function of the normal distribution $\\Phi(S; \\mu, \\sigma^2)$ as follows. \n",
    "    \\begin{align}\n",
    "    g(S) \n",
    "    &= p \\cdot \\int_{S}^{\\infty} (x - S) \\cdot f(x) \\, dx + h \\cdot \\int_{-\\infty}^{S} (S - x) \\cdot f(x) \\, dx\\\\\n",
    "    &= p \\cdot \\int_{S}^{\\infty} x \\cdot f(x)\\, dx - p \\cdot \\int_{S}^{\\infty} S \\cdot f(x)\\, dx + h \\cdot \\int_{-\\infty}^{S} S \\cdot f(x) \\, dx - h \\cdot \\int_{-\\infty}^{S} x \\cdot f(x) \\, dx\\\\\n",
    "    &= p \\mathbb{E}[x] - (p+h) \\int_{-\\infty}^{S} x \\cdot f(x)\\, dx - S (p + (h-p) \\Phi(S; \\mu, \\sigma^2)) \\\\\n",
    "    &= p \\mathbb{E}[x] - (p+h) \\mathbb{E}[x|x<S] - S p + S (p-h) \\Phi(S; \\mu, \\sigma^2) \\\\\n",
    "    \\end{align}\n",
    "    We see that for any positive fixed $S$, $g(S)$ is increasing in $\\Phi(\\cdot)$ if $p-h$ is positive and decreasing in $\\Phi(\\cdot)$ if $p-h$ is negative. This reverses if $S$ is negative.\n",
    "\n",
    "    - Go back to our expression for the optimal $S$.\n",
    "    $$\n",
    "    \\Phi(S^*,\\mu,\\sigma^2) = \\frac{p}{p+h}\n",
    "    $$\n",
    "    Increasing $\\mu$ decreases $\\Phi(S;\\mu,\\sigma^2)$ for all $S$ - so the optimum $S$ must be increased in order to hit the ratio $\\frac{p}{p+h}$. Increasing $\\sigma^2$ makes $\\Phi(\\cdot)$ increase more slowly. By the symmetry of the normal distribution around the mean, for any $s^2 > \\sigma^2$ we will have $\\Phi(S,\\mu,\\sigma^2) > \\Phi(S,\\mu,s^2)$ for $S < \\mu$ and vice versa for $S > \\mu$. As a result, the impact on the optimal $S^*$ is unclear and will depend on $p$ and $h$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer\n",
    "\n",
    "1. We can frame the milk vendor's problem as a portfolio of options.\n",
    "    -  The underlying asset is demand $x$, and the milk vendor holds options with a strike price at the supply $S$ that they hold.\n",
    "    - The cost $g_1(S)$ is positive when demand is too high - so it is equivalent to shorting a call option on demand. The cost $g_2(S)$ is positive when demand is too low - so it is equivalent to shorting a put option on demand. \n",
    "    - $p$ and $h$ represent the position on these options the milk vendor is holding.\n",
    "\n",
    "2. With this framing, the milk vendor's problem is equivalent to setting the optimal strike price when given a fixed position of options. The math to do this is much the same as what we have done in the previous parts of this problem. The insights from financial engineering are probably much more useful if the milk vendor is solving some type of dynamic stocking problem, where they can order milk at fixed frequencies and need to solve the dynamic stocking problem. Then we can apply much of the same lessons from dynamic programming or RL to this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (D) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import integrate\n",
    "\n",
    "'''\n",
    "### Part (D): Numerical Solution and Simulation\n",
    "\n",
    "fill in the code below, and then run the code in the next cell for the simulation\n",
    "'''\n",
    "\n",
    "# simulation parameters\n",
    "p = 5  # Cost of understocking (per unit)\n",
    "h = 2  # Cost of overstocking (per unit)\n",
    "lambda_param = 1.0  # Parameter for exponential distribution\n",
    "\n",
    "# Probability density function for demand\n",
    "def demand_pdf(x):\n",
    "    return (lambda_param * np.exp(-lambda_param * x))\n",
    "\n",
    "# Cumulative distribution function for demand\n",
    "def demand_cdf(x):\n",
    "    return (1-np.exp(-lambda_param * x))\n",
    "\n",
    "# Expected cost function g(S)\n",
    "def expected_cost(S):\n",
    "    # g1(S): Understocking cost\n",
    "    g1 = p*(integrate.quad(lambda x: x*demand_pdf(x), S, np.inf)[0] - S * (1-demand_cdf(S)))\n",
    "\n",
    "    # g2(S): Overstocking cost (integral using CDF)\n",
    "    g2 = h*(S * demand_cdf(S) - integrate.quad(lambda x: x*demand_pdf(x), 0, S)[0])\n",
    "    \n",
    "    return g1 + g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.float64(1.2527630511681802)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = minimize_scalar(expected_cost, bounds=(0, 10), method='bounded')\n",
    "optimal_S = result.x\n",
    "\n",
    "# Simulation of costs\n",
    "S_values = np.linspace(0, 10, 500)\n",
    "costs = [expected_cost(S) for S in S_values]\n",
    "\n",
    "# Plotting the costs against S\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(S_values, costs, label=\"Expected Cost $g(S)$\", color='blue')\n",
    "plt.axvline(optimal_S, color='red', linestyle='--', label=f\"Optimal $S^* \\\\approx {optimal_S:.2f}$\")\n",
    "plt.xlabel(\"Supply $S$\")\n",
    "plt.ylabel(\"Expected Cost $g(S)$\")\n",
    "plt.title(\"Expected Cost $g(S)$ vs. Supply $S$ (Exponential Demand)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "optimal_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Optimal Bank Cash Management with Risky Investments and Penalties (Led By: Junyoung)\n",
    "\n",
    "Assume you are the owner of a bank where customers come in randomly every day to make cash deposits and to withdraw cash from their accounts. At the end of each day, you can borrow (from another bank, without transaction costs) any cash amount $y > 0$ at a constant daily interest rate $R$, meaning you will need to pay back a cash amount of $y(1 + R)$ at the end of the next day. Also, at the end of each day, you can invest a portion of your bank’s cash in a risky (high return, high risk) asset. Assume you can change the amount of your investment in the risky asset each day, with no transaction costs (this is your mechanism to turn any amount of cash into risky investment or vice-versa).\n",
    "\n",
    "A key point here is that once you make a decision to invest a portion of your cash in the risky asset at the end of a day, you will not have access to this invested amount as cash that otherwise could have been made available to customers who come in the next day for withdrawals. More importantly, if the cash amount $c$ in your bank at the start of a day is less than $C$, the banking regulator will make you pay a penalty of $K \\cdot \\cot\\left( \\frac{\\pi c}{2C} \\right)$ (for a given constant $K > 0$).\n",
    "\n",
    "For convenience, we make the following assumptions:\n",
    "- Assume that the borrowing and investing is constrained so that we end the day (after borrowing and investing) with positive cash ($c > 0$) and that any amount of regulator penalty can be immediately paid (meaning $c \\geq K \\cdot \\cot\\left( \\frac{\\pi c}{2C} \\right)$ when $c \\leq C$).\n",
    "- Assume that the deposit rate customers earn is so small that it can be ignored.\n",
    "- Assume for convenience that the first half of the day is reserved for only depositing money and the second half of the day is reserved for only withdrawal requests.\n",
    "- Assume that if you do not have sufficient cash to fulfill a customer withdrawal request, you ask the customer to make the withdrawal request again the next day.\n",
    "- Assume all quantities are continuous variables.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (A): MDP Modeling\n",
    "\n",
    "Model the bank’s problem as a **Markov Decision Process (MDP)** with the goal of maximizing the **Expected Utility of assets less liabilities** at the end of a $T$-day horizon, conditional on any current situation of assets and liabilities.\n",
    "\n",
    "1. **State Space**: Define the possible states of the system.\n",
    "2. **Action Space**: Specify the possible actions available to the bank at each state.\n",
    "3. **Transition Function**: Describe how the state evolves based on the current state and the chosen action.\n",
    "4. **Reward Function**: Specify the reward structure that incentivizes optimal behavior.\n",
    "\n",
    "*Note*: Be very careful with your notation; ensure that every subscript, index, superscript, prime, etc. is properly defined and necessary. There are a lot of variables at play, so everything must be properly defined or points will be deducted.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (B): Reinforcement Learning Approach\n",
    "\n",
    "In a practical setting, we do not know the exact probability distributions of the customer deposits and withdrawals. Neither do we know the exact stochastic process of the risky asset. But assume we have access to a large set of historical data detailing daily customer deposits and withdrawal requests, as well as daily historical market valuations of the risky asset. Assume we also have data on new customers as well as leaving customers (sometimes due to their withdrawal requests not being satisfied promptly).\n",
    "\n",
    "Describe your approach to solving this problem with **Reinforcement Learning** by using the historical data described above.\n",
    "\n",
    "1. Specify which **Reinforcement Learning algorithm** you would use, including any customizations for this problem.\n",
    "2. Provide sufficient detail that will enable a programmer with knowledge of RL to implement your ideas.\n",
    "\n",
    "*Note*: You are not expected to write any code for this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "First, I rewrite the problem in more precise notation. A time period $t$ consists of the following stages for all $t \\in [0,T-1] \\cap \\mathbb{N}$.\n",
    "1. The end of the day after the bank has paid off all of its loans, and received the returns on its risky investment. I will denote the cash that the bank holds at this moment as its assets $A_t$. I also denote the total deposits at the bank at this moment as $\\Delta_t$ - the total amount the bank owes to its customers. \n",
    "2. The bank chooses its portfolio for the next day by borrowing $y_t \\geq 0$ and investing $x_t \\geq 0$ into the risky asset. The bank also holds $c_t > 0$ in cash. \n",
    "    -  I assume the risky asset has returns $\\theta_t$ with distribution defined by the cumulative distribution function $F^\\theta(\\theta)$ which is bounded below at $0$. This assumption implies the bank can not lose more than its investment into the risky asset (limited liability).\n",
    "    - I also assume the bank can not short the risky asset. \n",
    "3. Customers come in to deposit their money. I will denote the total deposits gained for the day as $D_t \\geq 0$. \n",
    "    - I assume $\\tilde{D}_t$ has distribution defined by the cumulative distribution function $F^D(D)$.\n",
    "4. Customers come in to withdraw their money where $W_t \\geq 0$ will denote the total withdrawals in the day. \n",
    "    - I assume $\\tilde{W}_t$ has distribution defined by the cumulative distribution function $F^W(W; \\Delta)$.\n",
    "    - If the bank has insufficent cash to cover the withdrawal, the customer will come again the next day. Let the amount of deposits demanded by customers due to customers being turned away in the previous day be denoted $w_t$. \n",
    "    - Note that the distribution is a function of $\\Delta$ because the support of $F^W$ is bounded above at $\\Delta$ since customers can not withdraw more than the quantity deposited. \n",
    "    - Note that this for simplicity, I assume that the distribution of customers coming to deposit and withdraw remains consistent over times and states. This may not be the case in reality as rejecting large withdrawal requests could trigger a bank run which may change the distribution of deposits over time ands states. This could be amended by allowing for the distributions $F^D$ and $F^W$ to be functions of additional variables that would then have to be included as state variables.\n",
    "5. Finally, the end of the day comes, and the bank repays the amount borrowed with interest $y_t (1+R)$, receives the return on its risky investment $x_t \\cdot \\theta_t$, and pays any penalties it owes.\n",
    "This concludes time period $t$. The terminal period $T$ will have the bank owner return all of the deposits $\\Delta_T$ and consume the remaining balance. Under the way we have described the problem, this will exactly correspond to assets less liabilities since the bank will have already paid off its loans, received the returns on its risky investment, and paid any penalties at the end of period $T-1$. I maintain the assumption that the bank will need to have a positive cash balance on the final day - so all depositor's are guaranteed to have their funds returned. \n",
    "\n",
    "Now to answer the question:\n",
    "1. The state variables are the assets of the bank $A_t \\in (0, \\infty)$, the total deposits the bank owes customers $\\Delta_t \\in [0,\\infty)$, the total amount of withdrawals rejected on the previous day $w_t \\in [0, \\infty)$, and the time $t$. The game terminates at time $T$, so any state with time $t=T$ is a terminal state. \n",
    "\n",
    "2. The bank has two actions $y_t \\in \\mathbb{R}_+$ and $x_t \\in \\mathbb{R}_+$ where $y_t$ is the amount to borrow and $x_t$ is the amount to invest in the risky asset. The amount of cash the bank holds in reserve $c_t$ can be backed out as $c_t = A_t + y_t - x_t$. The bank's choice of $y_t$ and $x_t$ is constrained such that $c_t \\geq K \\cdot \\cot\\left(\\frac{\\pi c_t}{2C}\\right)$ if $c_t < C$.\n",
    "\n",
    "3. Given assets $A_t$ at beginning of period $t$, suppose the bank chooses to borrow $y_t$ and invest $x_t$ in the risky asset. The state variable for time increments by $1$ and the other state variables follow the laws of motion detailed below.\n",
    "    $$\n",
    "        A_{t+1} = c_t - \\mathbb{I}[c_t < C] \\cdot K \\cdot \\cot\\left( \\frac{\\pi c_t}{2C} \\right) + D_t - \\max\\{W_t + w_t, c_t - \\mathbb{I}[c_t < C] \\cdot K \\cdot \\cot\\left( \\frac{\\pi c_t}{2C} \\right)+ D_t\\} - y_t(1+R) + x_t \\theta_t\\\\\n",
    "        \\Delta_{t+1} = \\Delta_t + D_t - (W_t-w_t)\\\\\n",
    "        w_{t+1} = \\max\\{0, W_t + w_t - c_t + y_t + D_t\\}\n",
    "    $$\n",
    "    where $\\mathbb{I}$ is the indicator function. \n",
    "\n",
    "4. I assume that the bank is maximizing the expected utility of assets less liabilities at the end of day $T$.\n",
    "    $$\n",
    "    U\\left(A_T - \\Delta_T\\right)\n",
    "    $$\n",
    "    I think there are two choices in how to structure the rewards. You can let the reward only be the termination value of the game, $A_T - \\Delta_T$ and let there be no immediate rewards. \n",
    "\n",
    "    Alternatively, the algorithm probably performs better if you give it more immediate rewards. The bank's net assets will change every day, and this change will eventually be reflected in the termination value. To see this:\n",
    "    $$\n",
    "    A_T - \\Delta_T = (A_T - A_{T-1})+ A_{T-1} - (\\Delta_T - \\Delta_{T-1}) + \\Delta_{T-1} = \\cdots\n",
    "    $$\n",
    "    Extending this logic shows that summing the change in the net assets every day will yield the final net assets number $A_T - \\Delta_T$. Therefore, we can set the reward function to be the daily change in net assets.\n",
    "    $$\n",
    "    R_t = (A_t - \\Delta_t) - (A_{t-1} - \\Delta_{t-1})\n",
    "    $$\n",
    "\n",
    "Therefore we can write the Bellman for this problem as follows.\n",
    "\\begin{align}\n",
    "    V(A_t,\\Delta_t, w_t, t) = \\max\\limits_{y_t,x_t} (A_t - \\Delta_t) - (A_{t-1} - \\Delta_{t-1}) + \\mathbb{E}\\left[V(A_{t+1}, \\Delta_{t+1}, w_{t+1}, t+1)| y_t, x_t\\right]\n",
    "\\end{align}\n",
    "where our variables follow the laws of motion described above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "Given that in the real world we do not know the exact distributions of customer deposits, withdrawals or the returns of the risky asset ($F^D,F^W,F^\\theta$ described in the model above), reinforcement learning is a reasonable way to attempt to solve this problem. \n",
    "\n",
    "With access to historical data on realizations of customer deposits, withdrawals, and returns, we can generate a sampling model of the transition models and transition samples. Furthermore, we can add the data on new and leaving customers as well to better estimate the sampling model on how actions affect the distribution of future deposits and withdrawals (i.e. we can think of number of customers as a state variable as well). I will provide more explicit details on how to construct these items below. Note that we can think about this as either simulating the environment (estimating the transition probabilities) or just estimating the distribution of values as a function of the states using our data. \n",
    "\n",
    "1. I will describe how Deep Q learning (by which I mean temporal-difference with batch updating and experience-replay) can be used to solve this problem. Deep Q learning is a good fit because it addresses the following problems of using a stream of historical data to predict bank returns.\n",
    "    - Unless the bank has an explicit time horizon in mind, there is no terminal state other than bankruptcy - and this is not something the bank would be able to experiment with. Hence it is not possible to provide full episodic traces, meaning that we can not use a MC method. \n",
    "    - There is high autocorrelation over time in the flow of deposits, withdrawals, and returns. The experience-replay component of Deep Q learning helps address this issue by randomly sampling from the data bank.\n",
    "    - Even though we have many data points, there are often rare events in the economy that can significantly change the bank's value function. This is an issue that is somewhat addressed by batch updating and experience-replay as it prevents the algorithm from overupdating on the present (i.e. the economy is good for a long stretch of time and biases the algorithm towards a good economy, but experience-replay helps prevent this by sampling from previous bad stretches of the economy and batch updating prevents the algorithm from updating only on recent data.)\n",
    "2. We can implement this algorithm in the following steps (very closely following the description in the slides for batch RL)\n",
    "    - If we closely followed the model described in part 1, we would use the total assets of the bank, the deferred withdrawals and the time remaining as the state variables. However, there are a few issues translating this into a real world implementation.\n",
    "        - What time would we want to set as the termination time? One simple solution could be setting $T$ to 20 years or somethign relatively far away. More involved ways to approach this problem is to instead assume an infinite horizon and have fixed dividends paid out from the net assets of the bank that the bank's owners can immediately consume. Then the reward function would be to maximize the discounted sum of these dividends rather than the net assets at time $T$. \n",
    "        - It is likely that there are way more states that are important to this problem in the real world than the stylized model of part 1. For example, deposits, withdrawals, and returns are likely to be tightly related to a wide variety of macroeconomic factors. Therefore, it may be more realistic to simply throw the kitchen sink as the state variable - as even if you don't have data that explicitly captures a macroeconomic factor - you may be able to essentially recreate it using some combination of your data streams.\n",
    "    Thus a more realistic approach may be to set the state variable $S_t$ as the bank's assets, a few averages of deposits, withdrawals and returns of the risky asset over various time horizons (i.e. previous day, week, month, year), as well as time left until some arbitrary deadline. We could also drop the deadline entirely and simply use the reward function of the change in net assets described in the previous part. \n",
    "\n",
    "    However, since we have already fixed ideas in the previous part of the question, I will maintain the same notation. \n",
    "    - Given a state $S_t = (A_t, \\Delta_t, w_t, t)$ - take the action $A_t = (y_t,x_t)$ (amount of reserves $y_t$ and risky loan portfolio $x_t$) following an $\\epsilon$-greedy policy from the $Q$ network. The initial $Q$ values can be any arbitrary guess. These actions are ways to sample the state space.\n",
    "    - Observe the atomic experiences of a given state $S_t$, as well as the received the reward $A_t - \\Delta_t - (A_{t-1} - \\Delta_{t-1})$ and the following state $S_{t+1}$.\n",
    "    - Store these atomic experiences in replay memory\n",
    "    - Sample the replay memory to generate a batch to update the $Q$ network parameters\n",
    "    - Update the $Q$ network as follows:\n",
    "    $$\n",
    "    \\text{Change in } \\omega = \\alpha \\cdot \\sum\\limits_{i} (R_i + \\gamma \\cdot \\max\\limits_{A_i'} Q(S_i', A_i'; \\omega^{-}) - Q(S_i, A_i; \\omega)) \\cdot \\nabla_\\omega Q(S_i, A_i, \\omega)\n",
    "    $$\n",
    "    where the prime indicates the variable in the next period and $w^{-}$ indicates the \"frozen\" parameters for the $Q$ network that are not updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Career Optimization (Led By: Junyoung)\n",
    "\n",
    "Imagine you live in a world where every job is an hourly-wage job. You have $H$ available hours in a day (for some fixed $H \\in \\mathbb{Z}^+$), and each morning when you leave your house, you can decide to split those $H$ hours into:\n",
    "\n",
    "- Hours spent on learning to get better at your current job (call it $l \\in \\mathbb{Z}_{\\geq 0}$),\n",
    "- Hours spent on searching for another job (call it $s \\in \\mathbb{Z}_{\\geq 0}$), and\n",
    "- Remaining $H - l - s$ hours spent on actually working on your current job.\n",
    "\n",
    "If your job currently pays you at an hourly-wage of $w$ dollars, then at the end of that day, you will be given a cash amount of $w \\cdot (H - l - s)$ dollars. We assume that any hourly-wage $w$ in our world is an integer in the finite set $\\{1, 2, \\dots, W\\}$ for some fixed $W \\in \\mathbb{Z}^+$.\n",
    "\n",
    "Each employer has a wage model such that if you spend $l$ hours on learning on a given day where your hourly-wage was $w$, then the employer sends you an email the next morning with that new day’s hourly-wage of:  \n",
    "$$\\min(w + x, W)$$  \n",
    "where $x$ is a Poisson random variable with mean $\\alpha \\cdot l$ for some fixed $\\alpha \\in \\mathbb{R}^+$.\n",
    "\n",
    "Each morning, with probability $\\frac{\\beta s}{H}$ for some fixed $\\beta \\in [0, 1]$, you will receive an email from another employer with a job-offer with hourly-wage of  \n",
    "$$\\min(w + 1, W)$$  \n",
    "where $w$ was the hourly wage of the job you were on the previous day and $s$ is the number of hours you spent on job-search the previous day.\n",
    "\n",
    "You read all your emails before you leave your house in the morning. If another job is offered to you and if the hourly-wage of that job is greater than your current employer’s hourly-wage stated in that morning’s email, then you accept the other job. Otherwise, you continue in your current job. Whichever job you decide to do, each morning when you leave your house, you decide how to split the $H$ hours of that day into learning hours, job-searching hours, and working hours.\n",
    "\n",
    "Your goal is to maximize the **Expected (Discounted) Wages** earned over an infinite horizon (assume you never age and will live infinitely). The daily discount factor is a fixed $0 < \\gamma < 1$.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (A): MDP Modeling\n",
    "\n",
    "With proper mathematical notation, model this as a **Finite MDP** specifying the states, actions, rewards, state-transition probabilities, and discount factor. Be very precise with your notation!!\n",
    "\n",
    "---\n",
    "\n",
    "### Part (B): Python Implementation\n",
    "\n",
    "Implement this MDP in Python. If you wish, you may use the code in the git repo that you forked at the start of the course (e.g., `FiniteMarkovDecisionProcess`), but if you prefer, you can implement it from scratch or use any code you have written for the course previously (whichever is more convenient for you).\n",
    "\n",
    "---\n",
    "\n",
    "### Part (C): Solving for the Optimal Policy\n",
    "\n",
    "Solve for the **Optimal Value Function** and **Optimal Policy** using **Value Iteration**. If you wish, you may use the code in the git repo that you forked at the start of the course (e.g., `rl/dynamic_programming.py`), but if you prefer, you can implement it from scratch or use any code you have written for the course previously (whichever is more convenient for you).\n",
    "\n",
    "---\n",
    "\n",
    "### Part (D): Visualization\n",
    "\n",
    "Plot a graph of the **Optimal Policy** (or print the optimal policy) for the following configuration:  \n",
    "$H = 10$, $W = 30$, $\\alpha = 0.08$, $\\beta = 0.82$, $\\gamma = 0.95$.  \n",
    "\n",
    "Provide an intuitive explanation for this optimal policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "I will again restate the problem more precisely to pin notation and concepts. In every period $t$, the following occurs.\n",
    "1. I denote the start of the period to be right after you have read your emails - so if you have a received a better wage or a job offer from another employer that improves your wage you have already accepted the job. Let $w_t$ be the wage after you read your emails.\n",
    "2. You leave your house and choose your action $a_t = (l_t, s_t)$, your hours spent on learning about your current job and the number of hours spent searching for another job.\n",
    "4. You receive reward $R_t = w_t \\cdot (H - l_t - s_t)$ for working your job\n",
    "5. You go home and sleep after a day of work. Once again you check your email in the morning. You receive your raise $r_t = \\min\\{w_t+x_t, W\\}$ where $x_t \\sim Poisson(a \\cdot l_t)$. Furthermore, with probability $\\frac{\\beta \\cdot s_t}{H}$, you receieve a job offer from another employer with offer $o_t = \\min\\{w_t+1, W\\}$. \n",
    "6. Your wage for the next day is set to $w_{t+1} = \\max\\{o_t, r_t\\}$, where $\\mathbb{I}$ is an indicator function for whether or not you have received a job offer. \n",
    "\n",
    "This concludes time period $t$.\n",
    "\n",
    "To answer the question:\n",
    "1. The state is your current wage $w_t \\in \\{1,2,\\cdots, W\\}$. The game technically does not have a terminal state, but once you have a wage $W$ the game no longer evolves. If your current wage is $W$, there is no longer a point to learning more about your job or searching for a new one, so you simply work the full $H$ hours you have every day. This implies that your total future reward is $\\frac{W \\cdot H}{1-\\gamma}$ which is the geometric sum of the wage of fully working $W \\cdot H$ discounted by $\\gamma$. Therefore we can treat $W$ as a terminal state with reward $\\frac{W \\cdot H}{1-\\gamma}$.\n",
    "2. The action $a_t = (l_t,s_t) \\in \\{1,2,\\cdots,H\\} \\times \\{1,2,\\cdots H-l_t\\}$ is your choice of time allocation to learning or searching.\n",
    "3. The reward in every period is the wage you receive $R_t = w_t \\cdot (H-l_t-s_t)$. \n",
    "4. As previously mentioned, $w_t = W$ is a terminal state so the transition probabilities are $0$. Given a state $w_t < W$ and action $a_t = (l_t,s_t)$, let $P(w_{t+1} | w_t, a_t)$ denote the state transition probability conditional on the state and actions that the next state is $w_{t+1}$. The probability that you get a new wage of $w_{t+1} = w_t + 1$ is the probability that you get a raise of $1$ plus the probability that you do not get a raise from your employer but do get a job offer. \n",
    "\\begin{align}\n",
    "P(w_{t+1} = w_t + 1 | w_t, a_t) \n",
    "&= (a\\cdot l_t) e^{-a \\cdot l_t} + \\left(1-(a\\cdot l_t) e^{-a \\cdot l_t}\\right) \\frac{\\beta \\cdot s_t}{H}\\\\\n",
    "\\end{align}\n",
    "    The probability of getting a new wage of $w_{t+1} = w_t + x_t$ for $x_t > $W - w_t$ is $0$ and for $x_t \\leq W - w_t$ is given below.\n",
    "\\begin{align}\n",
    "P(w_{t+1} = w_t + x_t | w_t, a_t) &= \\frac{(a\\cdot l_t)^{x_t} e^{-a \\cdot l_t}}{x_t!} \n",
    "\\end{align}\n",
    "5. The discount factor is $\\gamma < 1$.\n",
    "\n",
    "We can write the Bellman for this problem as follows.\n",
    "\\begin{align}\n",
    "    V(w_t) = \\max\\limits_{l_t,s_t} &  w_t(H - l_t-s_t) + \\gamma \\mathbb{E}\\left[V(w_{t+1}) | l_t,s_t\\right]\n",
    "\\end{align}\n",
    "where the expectation is over the probabilities of receiving a job offer or a raise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for our state variable\n",
    "@dataclass(frozen=True)\n",
    "class COState:\n",
    "    wage: int\n",
    "\n",
    "# Define a mapping from states to a mapping of rewards to transition probabilities\n",
    "StateMapping = Mapping[\n",
    "    COState,\n",
    "    Mapping[float, Categorical[Tuple[COState, float]]]\n",
    "]\n",
    "\n",
    "# Define our MDP\n",
    "class CareerMDP(FiniteMarkovDecisionProcess[COState, float]):\n",
    "\n",
    "    '''\n",
    "    W is the maximum wage\n",
    "\n",
    "    H is the maximum number of hours in a day\n",
    "\n",
    "    aParam is the parameter for raises\n",
    "\n",
    "    betaParam is the parameter for job offers\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        W: int,\n",
    "        H: int,\n",
    "        aParam: float,\n",
    "        betaParam: float,\n",
    "        gammaParam: float\n",
    "    ):\n",
    "        self.W: int = W\n",
    "        self.H: int = H\n",
    "        self.aParam: float = aParam\n",
    "        self.betaParam: float = betaParam\n",
    "        self.gammaParam: float = gammaParam\n",
    "\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "\n",
    "    # Function to generate the mapping from states to mappings from actions to transition probabilities\n",
    "    def get_action_transition_reward_map(self) -> StateMapping:\n",
    "        d: Dict[COState, Dict[float, Categorical[Tuple[COState,float]]]] = {}\n",
    "\n",
    "        W = self.W\n",
    "        H = self.H\n",
    "        a = self.aParam\n",
    "        beta = self.betaParam\n",
    "        gamma = self.gammaParam\n",
    "\n",
    "        for w in range(W):\n",
    "            state: COState = COState(w)\n",
    "            d1: Dict[float, Categorical[Tuple[COState, float]]] = {}\n",
    "\n",
    "            for l in range(H):\n",
    "                lambdaParam = a*l\n",
    "                for s in range(H-l):\n",
    "                    action=tuple([l,s])\n",
    "                    reward: float = w*(H-l-s)\n",
    "\n",
    "                    if w < W-1:\n",
    "                        # Set the transition probability to $0$ for wages less than w\n",
    "                        sr_probs_dict: Dict[Tuple[COState, float], float] =\\\n",
    "                        {(COState(i), reward): 0 for i in range(w-1)}\n",
    "                        \n",
    "                        # Set the transition probability for moving to wages in [w+2, W]\n",
    "                        sr_probs_dict: Dict[Tuple[COState, float], float] =\\\n",
    "                        {(COState(w+x), reward): \n",
    "                            lambdaParam**x*np.exp(-lambdaParam)/math.factorial(x)\n",
    "                            for x in range(w+2,W-w+1)}\n",
    "                        \n",
    "                        # Set the transition probability for moving to wage w+1\n",
    "                        sr_probs_dict[(COState(w+1), reward)] = lambdaParam*np.exp(-lambdaParam) +\\\n",
    "                              (1-lambdaParam*np.exp(-lambdaParam))*((beta*s)/H)\n",
    "\n",
    "                        # Set the transition probability for moving to wage w\n",
    "                        sr_probs_dict[(COState(w), reward)] = np.exp(-lambdaParam)*(1-(beta*s)/H)\n",
    "\n",
    "                    if w == W-1:\n",
    "                        # Set the transition probability to $0$ for wages less than w\n",
    "                        sr_probs_dict: Dict[Tuple[COState, float], float] =\\\n",
    "                        {(COState(i), reward): 0 for i in range(w-1)}\n",
    "\n",
    "                        # Set the transition probability for moving to wage W\n",
    "                        sr_probs_dict[(COState(w+1), reward)] = lambdaParam*np.exp(-lambdaParam) +\\\n",
    "                              (1-lambdaParam*np.exp(-lambdaParam))*((beta*s)/H)\n",
    "\n",
    "                        # Set the transition probability for moving to wage w\n",
    "                        sr_probs_dict[(COState(w), reward)] = np.exp(-lambdaParam)*(1-(beta*s)/H)\n",
    "\n",
    "                    if w == W: # The terminal state\n",
    "                        # A little strange because I want to assign a reward to it\n",
    "                        # Set the transition probabilities to $0$\n",
    "                        sr_probs_dict: Dict[Tuple[COState, float], float] =\\\n",
    "                        {(COState(i), ((H-l-s)*W)/(1-gamma)): 0 for i in range(W+1)}\n",
    "                        # Essentially, I assume the one shot deviation principle\n",
    "\n",
    "                    d1[action]=Categorical(sr_probs_dict)\n",
    "\n",
    "            d[state] = d1\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our parameters\n",
    "user_W = 10\n",
    "user_H = 30\n",
    "user_a = 0.08\n",
    "user_beta = 0.82\n",
    "user_gamma = 0.95\n",
    "\n",
    "# Call our MDP\n",
    "si_mdp: FiniteMarkovDecisionProcess[COState, Tuple[int,int]] =\\\n",
    "    CareerMDP(\n",
    "        W = user_W,\n",
    "        H = user_H,\n",
    "        aParam = user_a,\n",
    "        betaParam = user_beta,\n",
    "        gammaParam = user_gamma\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (D) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP Value Iteration Optimal Value Function and Optimal Policy\n",
      "--------------\n",
      "{NonTerminal(state=COState(wage=1)): np.float64(4011.3171957046634),\n",
      " NonTerminal(state=COState(wage=0)): np.float64(3880.944006380311),\n",
      " NonTerminal(state=COState(wage=2)): np.float64(4114.053748637487),\n",
      " NonTerminal(state=COState(wage=3)): np.float64(4026.365479197543),\n",
      " NonTerminal(state=COState(wage=4)): np.float64(4118.743070003736),\n",
      " NonTerminal(state=COState(wage=5)): np.float64(4386.908042080716),\n",
      " NonTerminal(state=COState(wage=6)): np.float64(4664.301401672663),\n",
      " NonTerminal(state=COState(wage=7)): np.float64(4932.3927285301415),\n",
      " NonTerminal(state=COState(wage=8)): np.float64(5182.222036098581),\n",
      " NonTerminal(state=COState(wage=9)): np.float64(5399.999813876358)}\n",
      "For State COState(wage=0): Do Action (29, 0)\n",
      "For State COState(wage=1): Do Action (29, 0)\n",
      "For State COState(wage=2): Do Action (29, 0)\n",
      "For State COState(wage=3): Do Action (29, 0)\n",
      "For State COState(wage=4): Do Action (0, 29)\n",
      "For State COState(wage=5): Do Action (13, 0)\n",
      "For State COState(wage=6): Do Action (11, 0)\n",
      "For State COState(wage=7): Do Action (8, 0)\n",
      "For State COState(wage=8): Do Action (5, 0)\n",
      "For State COState(wage=9): Do Action (0, 0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rl.dynamic_programming import value_iteration_result\n",
    "\n",
    "print(\"MDP Value Iteration Optimal Value Function and Optimal Policy\")\n",
    "print(\"--------------\")\n",
    "opt_vf_vi, opt_policy_vi = value_iteration_result(si_mdp, gamma=user_gamma)\n",
    "pprint(opt_vf_vi)\n",
    "print(opt_policy_vi)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Car Sales (Led By: Junyoung)\n",
    "\n",
    "Imagine you own a car which you need to sell within a finite window of $N$ days. At the beginning of each day, you receive an offer from a dealership which is randomly distributed over the interval $[m, M]$, $0 < m < M$, with a known continuous distribution $Q$ on this support; the offers each day are i.i.d. After receiving an offer, you have to decide immediately whether to accept or reject it. If you reject the offer, it is lost, and you have to pay a parking cost for the car of $c \\geq 0$, which you must pay at the end of each day you do not sell the car. After $N$ days, the car has to be sold. The parameters $m$, $M$, and $c$ are all fixed positive real numbers. Your objective is to maximize the sale proceeds.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (A): MDP Modeling\n",
    "\n",
    "With proper mathematical notation, model this problem fully as an **MDP** by specifying the following:\n",
    "1. **States**: Define the state space\n",
    "2. **Actions**: Specify the possible actions available to the seller\n",
    "3. **Rewards**: Define the reward structure\n",
    "4. **State-Transition Probabilities**: Specify the transition dynamics\n",
    "5. **Discount Factor**: Indicate the discount factor\n",
    "\n",
    "Additionally, discuss what particular kind of MDP this is.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (B): Analytical Solution\n",
    "\n",
    "Solve this MDP analytically for the **optimal policy**. Provide a detailed explanation of the steps used to derive the policy and any key conditions or assumptions required (*Note*: this is to be done mathematically, not using code).\n",
    "\n",
    "---\n",
    "\n",
    "### Part (C): Special Case Analysis\n",
    "\n",
    "Consider the case where $c = 0$ and $Q(x) = U_{[m, M]}(x)$ (the uniform distribution on $[m, M]$). Solve for as closed-form a solution of the optimal policy **as possible**. To make this concrete, the functional form of your optimal policy should be explicitly defined but can depend on coefficients that are recursively defined. **You should not have integrals in your solution.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "I rewrite the problem to fix notation and concepts. A time period $t < T$ is defined as:\n",
    "1. You wake up on day $t$ of $T$ days. You receive an offer $x_t \\sim Q([m,M])$ from the dealership.\n",
    "2. You choose to either sell or continue. Let $z_{t+1} = 1$ if you sell the car on day $t$ and $0$ otherwise.\n",
    "3. If you continue, you pay the parking cost $c$. If you sell you receive $x_t$\n",
    "This concludes time period $t$. If it is time $T$ the game is over and the dealership no longer makes any offers.\n",
    "\n",
    "Therefore:\n",
    "1. The state is the vector $(x_t, t)$ with offer $x_t \\in [m,M]$, and time $t\\in \\mathbb{N}\\cap[0,T]$. The terminal states is any with $t = T$. \n",
    "2. The action is $z_{t}$, the choice to sell or not.\n",
    "3. The reward is $z_{t} x_t$. \n",
    "4. If you sell, the game ends and the transition probabilities to future states is $0$. If you do not sell, the transition probability of moving to state $(x')$ is $f(x')$ where $f$ is the probability density/mass function associated with $Q$. \n",
    "5. The discount factor is $\\gamma = 1$.\n",
    "\n",
    "We can write the Bellman for this problem as follows.\n",
    "\\begin{align}\n",
    "V(x_t, t) &= \\max\\limits_{z_{t}} z_{t} x_t + (1-z_{t})(\\mathbb{E}[V(x_{t+1},0,t+1)] - c)\n",
    "\\end{align}\n",
    "This can be viewed as two noteable variants of a MDP. First it is a finite tabular problem that can be solved recursively. Second, it is a stopping time problem where the action is to continue or stop. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "We solve this problem using backwards induction. At time $T-1$, you accept any offer made by the dealership.\n",
    "$$\n",
    "V(x_{T-1}, T-1) = x_{T-1}\n",
    "$$\n",
    "\n",
    "Therefore at time $T-2$:\n",
    "$$\n",
    "V(x_{T-2}, T-2) = \\max\\limits_{z_{T-1}} z_{T-1} x_{T-2} + (1-z_{T-1}) (\\mathbb{E}[x_{T-1}] - c)\n",
    "$$\n",
    "Thus you sell ($z_{T-1} = 1$) if $x_{T-2} \\geq  (\\mathbb{E}[x_{T-1}] - c)$. Substituting this policy in:\n",
    "$$\n",
    "V(x_{T-2}, T-2) = \\max\\limits_{z_{T-1}} \\max\\left\\{x_{T-2}, \\mathbb{E}[x_{T-1}] - c\\right\\}\n",
    "$$\n",
    "\n",
    "We can recursively solve for the value function in this manner for all $t$. Then the optimal policy is to sell if $x_t$ is greater than $\\mathbb{E}[V(x_{t+1},0,t+1)] - c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer\n",
    "\n",
    "Consider the case where $c = 0$ and $Q(x) = U_{[m, M]}(x)$ (the uniform distribution on $[m, M]$). Solve for as closed-form a solution of the optimal policy **as possible**. To make this concrete, the functional form of your optimal policy should be explicitly defined but can depend on coefficients that are recursively defined. **You should not have integrals in your solution.**\n",
    "\n",
    "In the special case where $c = 0$ and $Q(x) = U_{[m, M]}(x)$:\n",
    "\\begin{align}\n",
    "    V(x_{T-2}, T-2) \n",
    "    &= \\max\\limits_{z_{T-1}} \\max\\left\\{x_{T-2}, \\frac{M-m}{2}\\right\\}\n",
    "\\end{align}\n",
    "Then:\n",
    "\\begin{align}\n",
    "    V(x_{T-3}, T-3) \n",
    "    &= \\max\\limits_{z_{T-2}} z_{T-2} x_{T-3} + (1-z_{T-2}) \\mathbb{E} [V(x_{T-2},T-2)]\\\\\n",
    "    &= \\max\\limits_{z_{T-2}} z_{T-2} x_{T-3} + (1-z_{T-2}) \\frac{M-m}{4} + \\frac{3(M-m)}{8}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Constrained Consumption (Led By: Andrew Sung)\n",
    "\n",
    "Consider the following discrete-time MDP for constrained consumption. At $t = 0$, the agent is given a finite amount $x_0 \\in \\mathbb{R}^+$ of a resource. In each time period, the agent can choose to consume any amount of the resource, with the consumption denoted as $c \\in [0, x]$ where $x$ is the amount of the resource remaining at the start of the time period. This consumption results in a reduction of the resource at the start of the next time period:  \n",
    "$$x' = x - c.$$  \n",
    "\n",
    "Consuming a quantity $c$ of the resource provides a utility of consumption equal to $U(c)$, and we adopt the **CRRA utility function**:  \n",
    "$$\n",
    "U(c) = \\frac{c^{1 - \\gamma}}{1 - \\gamma}, \\quad (\\gamma > 0, \\gamma \\neq 1)\n",
    "$$\n",
    "\n",
    "Our goal is to maximize the aggregate discounted utility of consumption until the resource is completely consumed. We assume a discount factor of $\\beta \\in [0, 1]$ when discounting the utility of consumption over any single time period.\n",
    "\n",
    "We model this as a **discrete-time, continuous-state-space, continuous-action-space, stationary, deterministic MDP**, and so our goal is to solve for the **Optimal Value Function** and associated **Optimal Policy**, which will give us the optimal consumption trajectory of the resource. Since this is a stationary MDP, the **State** is simply the amount $x$ of the resource remaining at the start of a time period. The **Action** is the consumption quantity $c$ in that time period. The **Reward** for a time period is $U(c)$ when the consumption in that time period is $c$. The discount factor over each single time period is $\\beta$.\n",
    "\n",
    "We assume that the **Optimal Policy** is given by:  \n",
    "$$\n",
    "c^* = \\theta^* \\cdot x \\quad \\text{for some } \\theta^* \\in [0, 1].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Part (A): Closed-form Expression for $V_\\theta(x)$\n",
    "\n",
    "Our first step is to consider a fixed deterministic policy, given by:  \n",
    "$$c = \\theta \\cdot x \\quad \\text{for some fixed } \\theta \\in [0, 1].$$  \n",
    "Derive a closed-form expression for the **Value Function** $V_\\theta(x)$ for a fixed deterministic policy, given by $c = \\theta \\cdot x$. Specifically, you need to express $V_\\theta(x)$ in terms of $\\beta$, $\\gamma$, $\\theta$, and $x$.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (B): Solving for $\\theta^*$\n",
    "\n",
    "Use this closed-form expression for $V_\\theta(x)$ to solve for the $\\theta^*$ which maximizes $V_\\theta(x)$ (thus fetching us the **Optimal Policy** given by $c^* = \\theta^* \\cdot x$).\n",
    "\n",
    "---\n",
    "\n",
    "### Part (C): Expression for $V^*(x)$\n",
    "\n",
    "Use this expression for $\\theta^*$ to obtain an expression for the **Optimal Value Function** $V^*(x)$ in terms of only $\\beta$, $\\gamma$, and $x$.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (D): Validation of the Bellman Equation\n",
    "\n",
    "Validate that the **Optimal Policy** (derived in part B) and **Optimal Value Function** (derived in part C) satisfy the **Bellman Optimality Equation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "\n",
    "\n",
    "Under a fixed policy $c = \\theta \\cdot x$ the resource at time $t$ is \n",
    "\n",
    "$$x_t = (1 - \\theta)^t x$$\n",
    "\n",
    "so consumption at time $t$ is \n",
    "\n",
    "$$c_t = \\theta x_t = \\theta (1-\\theta)^tx.$$\n",
    "\n",
    "Thus, the total discounted utility is:\n",
    "\n",
    "\\begin{aligned}\n",
    "V_{\\theta}(x)\n",
    "&= \\sum_{t=0}^{\\infty} \\beta^t \\, U(c_t)\n",
    "= \\sum_{t=0}^{\\infty} \\beta^t \\,\\frac{\\bigl(\\theta\\,(1-\\theta)^t\\,x\\bigr)^{1-\\gamma}}{1-\\gamma}.\n",
    "\\\\[6pt]\n",
    "&= \\frac{x^{1-\\gamma}}{1-\\gamma}\\,\\theta^{\\,1-\\gamma}\n",
    "\\sum_{t=0}^{\\infty} \\Bigl[\\beta \\,(1-\\theta)^{\\,1-\\gamma}\\Bigr]^{t}.\n",
    "\\end{aligned}\n",
    "\n",
    "Since the sum is a geometric series with ratio $\\beta\\,(1-\\theta)^{1-\\gamma}$ (in absolute value < 1), it equals\n",
    "\n",
    "$$\\frac{1}{1 - \\beta\\,(1-\\theta)^{1-\\gamma}}.$$\n",
    "\n",
    "Hence, we have derived\n",
    "\n",
    "$$\\boxed{\n",
    "V_{\\theta}(x)\n",
    "= \\frac{x^{1-\\gamma}}{1-\\gamma} \n",
    "\\,\\frac{\\theta^{\\,1-\\gamma}}{1 - \\beta\\,(1-\\theta)^{\\,1-\\gamma}}.\n",
    "}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "\n",
    "We want $\\theta^* \\in [0,1]$ that maximizes $V_{\\theta}(x)$. A standard way (from Euler equations) is:\n",
    "\n",
    "- For CRRA utility, $U'(c)=c^{-\\gamma}$.\n",
    "- The optimal policy in steady consumption satisfies\n",
    "$$  U'(c_t) = \\beta\\,U'(c_{t+1}).$$\n",
    "\n",
    "- Here $c_t = \\theta x_t \\quad$ and $\\quad c_{t+1} = \\theta x_{t+1} = \\theta (1-\\theta) x_t$. Thus\n",
    "\n",
    "$$  (\\theta\\,x_t)^{-\\gamma} \n",
    "  = \\beta\\,\\bigl[\\theta(1-\\theta)x_t\\bigr]^{-\\gamma}.$$\n",
    "\n",
    "  Canceling out the common factors, we get\n",
    "$$  1 = \\beta\\,(1-\\theta)^{-\\gamma}\n",
    "  \\;\\;\\Longrightarrow\\;\\;\n",
    "  (1-\\theta)^{\\gamma} \n",
    "  = \\frac{1}{\\beta}\n",
    "  \\;\\;\\Longrightarrow\\;\\;\n",
    "  1-\\theta \n",
    "  = \\beta^{1/\\gamma}.$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\\boxed{\\theta^* = 1 - \\beta^{1/\\gamma}.}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer\n",
    "\n",
    "Substitute $\\theta^*$ back into $V_{\\theta}(x)$. Recall:\n",
    "\n",
    "$$V_{\\theta}(x)\n",
    "= \\frac{x^{1-\\gamma}}{1-\\gamma}\n",
    "\\,\\frac{\\theta^{1-\\gamma}}{1 - \\beta\\,(1-\\theta)^{1-\\gamma}}.$$\n",
    "\n",
    "At $\\theta=\\theta^* = 1 - \\beta^{\\,1/\\gamma}$, we have:\n",
    "\n",
    "- $(\\theta^*)^{\\,1-\\gamma} = [\\,1-\\beta^{1/\\gamma}\\,]^{\\,1-\\gamma}$,\n",
    "- $(1-\\theta^*) = \\beta^{1/\\gamma}$,\n",
    "- $\\beta\\,(1-\\theta^*)^{1-\\gamma} = \\beta\\,\\bigl(\\beta^{1/\\gamma}\\bigr)^{1-\\gamma}$\n",
    "= $\\beta^{1+\\frac{1-\\gamma}{\\gamma}} = \\beta^{\\frac{1}{\\gamma}}.$\n",
    "\n",
    "Therefore the denominator \n",
    "\n",
    "$$1 - \\beta\\,(1-\\theta^*)^{\\,1-\\gamma}\n",
    "= 1 - \\beta^{\\,1/\\gamma}.$$\n",
    "\n",
    "Putting it all together:\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "V^*(x)\n",
    "&= \\frac{x^{1-\\gamma}}{1-\\gamma}\n",
    "\\,(\\theta^*)^{\\,1-\\gamma}\n",
    "\\,\\frac{1}{\\,1 - \\beta\\,(1-\\theta^*)^{\\,1-\\gamma}}\\\\[6pt]\n",
    "&= \\frac{x^{1-\\gamma}}{1-\\gamma}\n",
    "\\,\\frac{\\bigl(1-\\beta^{1/\\gamma}\\bigr)^{\\,1-\\gamma}}\n",
    "{1 - \\beta^{1/\\gamma}}\\\\[6pt]\n",
    "&= \\frac{x^{1-\\gamma}}{1-\\gamma}\n",
    "\\,\\bigl[\\,1 - \\beta^{1/\\gamma}\\bigr]^{\\,1-\\gamma -1}\n",
    "= \\frac{x^{1-\\gamma}}{1-\\gamma}\n",
    "\\,\\bigl[\\,1 - \\beta^{1/\\gamma}\\bigr]^{-\\gamma}.\n",
    "\\end{aligned}\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\\boxed{\n",
    "V^*(x)\n",
    "= \\frac{x^{1-\\gamma}}{1-\\gamma}\n",
    "\\,\\Bigl(1 - \\beta^{1/\\gamma}\\Bigr)^{-\\gamma}.\n",
    "}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (D) Answer\n",
    "\n",
    "We verify that $V^*(x)$ and $c^*(x)=\\theta^*\\,x$ satisfy \n",
    "\n",
    "$$V^*(x)\n",
    "= \\max_{0\\le c\\le x}\\,\\Bigl\\{U(c) + \\beta\\,V^*(x-c)\\Bigr\\}.$$\n",
    "\n",
    "Under the proposed policy, $c^*(x)=\\theta^* x$, and $x-c^*(x)=\\beta^{1/\\gamma}x$.\n",
    "\n",
    "1. **Left side** is $V^*(x)$.  \n",
    "2. **Right side** for $c^*(x)$ is:\n",
    "   $$U\\bigl(\\theta^* x\\bigr) + \\beta\\,V^*\\bigl(\\beta^{1/\\gamma} x\\bigr).$$\n",
    "\n",
    "\n",
    "   - We have \n",
    "$$     U(\\theta^* x) = \\frac{(\\theta^* x)^{1-\\gamma}}{1-\\gamma}.$$\n",
    "\n",
    "   - By homogeneity of $V^*(x)$ (it scales like $x^{1-\\gamma}$), \n",
    "\n",
    "$$     V^*\\bigl(\\beta^{1/\\gamma} x\\bigr)\n",
    "     = \\bigl(\\beta^{1/\\gamma}\\bigr)^{\\,1-\\gamma}\\,V^*(x)\n",
    "     = \\beta^{\\frac{1-\\gamma}{\\gamma}}\\,V^*(x).$$\n",
    "\n",
    "   - Hence \n",
    "\n",
    "$$     \\beta\\,V^*(\\beta^{1/\\gamma} x) \n",
    "     = \\beta \\,\\beta^{\\frac{1-\\gamma}{\\gamma}}\\,V^*(x)\n",
    "     = \\beta^{\\,1+\\frac{1-\\gamma}{\\gamma}}\\,V^*(x)\n",
    "     = \\beta^{\\frac{1}{\\gamma}}\\,V^*(x).$$\n",
    "\n",
    "Putting these terms together and using \n",
    "$\\theta^* = 1-\\beta^{1/\\gamma}$,\n",
    "one can verify \n",
    "$$\\frac{(\\theta^* x)^{1-\\gamma}}{1-\\gamma}\n",
    "+ \\beta^{1/\\gamma}\\,V^*(x)\n",
    "= V^*(x),$$\n",
    "\n",
    "thereby satisfying the Bellman Optimality Equation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: TD and MC Algorithms (Led By: Andrew Sung)\n",
    "\n",
    "In this question, we explore the connection between **Temporal Difference (TD)** and **Monte Carlo (MC)** algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (A): Implementing TD($\\lambda$) Prediction Algorithm\n",
    "\n",
    "Implement the **TD($\\lambda$) Prediction algorithm** from scratch in Python code. First, implement it for the **Tabular case**. Next, implement it for the **Function Approximation case**.  \n",
    "\n",
    "Provide clear and well-commented code for both implementations, and describe any assumptions or simplifications made.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (B): Proof of MC Error as Sum of Discounted TD Errors\n",
    "\n",
    "Prove that the **MC Error** can be written as the sum of discounted TD errors, i.e.,  \n",
    "$$\n",
    "G_t - V(S_t) = \\sum_{u=t}^{T-1} \\gamma^{u-t} \\cdot \\big( R_{u+1} + \\gamma \\cdot V(S_{u+1}) - V(S_u) \\big)\n",
    "$$\n",
    "\n",
    "Work this out from scratch, rather than relying on general results from class or the textbook.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (C): Code Extension\n",
    "\n",
    "Extend `RandomWalkMRP` (in [rl/chapter10/random_walk_mrp.py](https://github.com/TikhonJelvis/RL-book/blob/master/rl/chapter10/random_walk_mrp.py)) to `RandomWalkMRP2D` which is a random walk in 2-D with states $\\{i, j) | 0 \\leq i \\leq B_1, 0 \\leq j \\leq B_2\\}$ with terminal states as $(0, j)$ and $(B_1, j)$ for all $j$, $(i, 0)$ and $(i, B_2)$ for all $i$, and with reward of 0 for all $(0, j)$ and for all $(i, 0)$, reward of 1 for all $(B_1, j)$ and for all $(i, B_2)$, and with discrete probabilities of 4 movements - UP, DOWN, LEFT, RIGHT from any non-terminal state. Analyze the convergence of MC and TD on this `RandomWalkMRP2D` much like how we analyzed it for `RandomWalkMRP`, along with plots of similar graphs.\n",
    "\n",
    "Only modify the code where the message `fill in` is noted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TD(lambda) estimate for the 2-state MDP: [1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# TD($\\lambda$) Prediction algorithm, tabular case\n",
    "import numpy as np\n",
    "\n",
    "def td_lambda_tabular(\n",
    "    num_states,\n",
    "    transitions,    # function or data structure to get (next_state, reward) from (state, action)\n",
    "    policy,         # function: policy(state) -> action\n",
    "    alpha=0.1,      # learning rate\n",
    "    gamma=0.99,     # discount factor\n",
    "    lambd=0.9,      # lambda parameter\n",
    "    num_episodes=500,\n",
    "    max_steps_per_episode=1000\n",
    "):\n",
    "    \"\"\"\n",
    "    TD(lambda) tabular implementation for state-value prediction.\n",
    "    :param num_states: number of states (assumes states are 0,1,2,...,num_states-1)\n",
    "    :param transitions: a function transitions(state, action) -> (next_state, reward)\n",
    "    :param policy: a function that gives the action for each state\n",
    "    :param alpha: learning rate\n",
    "    :param gamma: discount factor\n",
    "    :param lambd: lambda parameter\n",
    "    :param num_episodes: number of episodes to run\n",
    "    :param max_steps_per_episode: safety limit on episode length\n",
    "    :return: array of state values V\n",
    "    \"\"\"\n",
    "    V = np.zeros(num_states)  # state-value array, initialized to zero\n",
    "    for _ in range(num_episodes):\n",
    "        # 1. Generate an episode or run until terminal:\n",
    "        state = np.random.randint(num_states)  # or some initial state\n",
    "        # Initialize eligibility traces:\n",
    "        E = np.zeros(num_states)\n",
    "        \n",
    "        for step in range(max_steps_per_episode):\n",
    "            action = policy(state)\n",
    "            next_state, reward = transitions(state, action)\n",
    "            \n",
    "            # TD error\n",
    "            delta = reward + gamma * V[next_state] - V[state]\n",
    "            \n",
    "            # Update eligibility for current state\n",
    "            E[state] += 1.0\n",
    "            \n",
    "            # Update each state's value and eligibility:\n",
    "            for s in range(num_states):\n",
    "                V[s] += alpha * delta * E[s]\n",
    "                E[s] *= gamma * lambd\n",
    "            \n",
    "            # Move on\n",
    "            state = next_state\n",
    "                \n",
    "    return V\n",
    "\n",
    "def main_td_lambda_tabular():\n",
    "    \"\"\"\n",
    "    Demonstration/testing code: defines a small 2-state environment and runs td_lambda_tabular.\n",
    "    \"\"\"\n",
    "    # Define a 2-state environment:\n",
    "    # State 0 -> transitions to State 1 with reward +1\n",
    "    # State 1 -> terminal or stays in 1 with reward 0 (for simplicity, we just remain there).\n",
    "    def transitions_2state(state, action):\n",
    "        if state == 0:\n",
    "            # Move to state 1 with reward +1\n",
    "            return (1, 1.0)\n",
    "        else:\n",
    "            # Already in terminal or \"absorbing\" state => reward 0\n",
    "            return (1, 0.0)\n",
    "\n",
    "    # A trivial policy (only one action anyway)\n",
    "    def policy_2state(state):\n",
    "        return 0\n",
    "\n",
    "    num_states = 2\n",
    "    V_est = td_lambda_tabular(\n",
    "        num_states=num_states,\n",
    "        transitions=transitions_2state,\n",
    "        policy=policy_2state,\n",
    "        alpha=0.1,\n",
    "        gamma=0.9,\n",
    "        lambd=0.8,\n",
    "        num_episodes=1000,\n",
    "        max_steps_per_episode=10\n",
    "    )\n",
    "    \n",
    "    print(\"TD(lambda) estimate for the 2-state MDP:\", V_est)\n",
    "\n",
    "# Run the main function:\n",
    "main_td_lambda_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned weights: [1. 0.]\n",
      "Estimated V(0) = 0.9999999999999996\n",
      "Estimated V(1) = 0.0\n"
     ]
    }
   ],
   "source": [
    "# TD($\\lambda$) Prediction algorithm, function approximation \n",
    "import numpy as np\n",
    "\n",
    "def td_lambda_function_approx(\n",
    "    transitions,        # function transitions(state, action) -> (next_state, reward)\n",
    "    policy,             # function policy(state) -> action\n",
    "    phi,                # function phi(state) -> feature_vector\n",
    "    alpha=0.01,         # learning rate\n",
    "    gamma=0.99,         # discount factor\n",
    "    lambd=0.9,          # lambda\n",
    "    num_features=10,    # dimension of phi(s)\n",
    "    num_episodes=500,\n",
    "    max_steps_per_episode=1000\n",
    "):\n",
    "    \"\"\"\n",
    "    TD(lambda) with linear function approximation.\n",
    "    :return: parameter vector w (the approximate value function is w^T phi(s)).\n",
    "    \"\"\"\n",
    "    w = np.zeros(num_features)\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        # For a tiny 2-state MDP, let's just start at state=0 or random among (0,1).\n",
    "        state = 0  # or: np.random.choice([0,1])\n",
    "        \n",
    "        # Eligibility traces for weights:\n",
    "        E = np.zeros(num_features)\n",
    "        \n",
    "        for step in range(max_steps_per_episode):\n",
    "            x_s = phi(state)              # feature vector of current state\n",
    "            action = policy(state)\n",
    "            next_state, reward = transitions(state, action)\n",
    "            x_next = phi(next_state)\n",
    "            \n",
    "            v_s = np.dot(w, x_s)         # current state's value estimate\n",
    "            v_next = np.dot(w, x_next)   # next state's value estimate\n",
    "            \n",
    "            delta = reward + gamma * v_next - v_s\n",
    "            \n",
    "            # Update eligibility trace (accumulating):\n",
    "            E = gamma * lambd * E + x_s\n",
    "            \n",
    "            # Update parameters:\n",
    "            w += alpha * delta * E\n",
    "            \n",
    "            # Move on\n",
    "            state = next_state\n",
    "    \n",
    "    return w\n",
    "\n",
    "# To test functionality\n",
    "def main_td_lambda_function_approx():\n",
    "    \"\"\"\n",
    "    A small test to verify td_lambda_function_approx on a tiny 2-state MDP\n",
    "    using one-hot feature vectors.\n",
    "    \"\"\"\n",
    "    # (1) Define the 2-state environment transitions:\n",
    "    def transitions_2state(state, action):\n",
    "        # State 0 -> next_state=1 with reward=+1\n",
    "        # State 1 -> absorbing => remain in 1 with reward=0\n",
    "        if state == 0:\n",
    "            return (1, 1.0)\n",
    "        else:\n",
    "            return (1, 0.0)\n",
    "\n",
    "    # (2) Trivial policy (only one possible action)\n",
    "    def policy_2state(state):\n",
    "        return 0\n",
    "\n",
    "    # (3) One-hot features for states 0,1\n",
    "    def phi_2state(s):\n",
    "        # Only 2 features, so set num_features=2\n",
    "        # in the call to td_lambda_function_approx.\n",
    "        if s == 0:\n",
    "            return np.array([1.0, 0.0])\n",
    "        else:\n",
    "            return np.array([0.0, 1.0])\n",
    "\n",
    "    # (4) Run TD(lambda) with function approx:\n",
    "    w_est = td_lambda_function_approx(\n",
    "        transitions=transitions_2state,\n",
    "        policy=policy_2state,\n",
    "        phi=phi_2state,\n",
    "        alpha=0.1,      # bigger alpha to learn quickly\n",
    "        gamma=0.9,\n",
    "        lambd=0.8,\n",
    "        num_features=2, # matches phi_2state dimension\n",
    "        num_episodes=1000,\n",
    "        max_steps_per_episode=10\n",
    "    )\n",
    "    \n",
    "    # Interpret w_est as [V(0), V(1)]\n",
    "    print(\"Learned weights:\", w_est)\n",
    "    print(\"Estimated V(0) =\", w_est[0])\n",
    "    print(\"Estimated V(1) =\", w_est[1])\n",
    "\n",
    "# Run the demo\n",
    "main_td_lambda_function_approx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "Recall: \n",
    "$G_t = R_{t+1} + \\gamma\\,R_{t+2} + \\gamma^2\\,R_{t+3} + \\dots + \\gamma^{T-1 - t} R_T$.\n",
    "\n",
    "\n",
    "We want to show:\n",
    "$G_t - V(S_t) \n",
    "\\;=\\;\n",
    "\\sum_{u=t}^{T-1}\\,\\gamma^{u - t}\\,\\bigl[\\,R_{u+1} + \\gamma\\,V(S_{u+1}) - V(S_u)\\bigr]$.\n",
    "\n",
    "$\\newline$\n",
    "$\\newline$\n",
    "\n",
    "Step 1. Expand the right-hand side:\n",
    "\n",
    "$$\\sum_{u=t}^{T-1} \\gamma^{u - t}\n",
    "\\bigl[R_{u+1} + \\gamma\\,V(S_{u+1}) - V(S_u)\\bigr]$$\n",
    "\n",
    "$$ = \\sum_{u=t}^{T-1} \\gamma^{u-t} R_{u+1} + \\gamma \\sum_{u=t}^{T-1} \\gamma^{u-t} V(S_{u+1}) - \\sum_{u=t}^{T-1} \\gamma^{u-t} V(S_u) $$\n",
    "\n",
    "$\\newline$\n",
    "$\\newline$\n",
    "Step 2. Rewrite sums with index shifts:\n",
    "\n",
    "Consider $\\sum_{u=t}^{T-1} \\gamma^{u - t + 1} V(S_{u+1})$. \n",
    "Let $m = u + 1$. \n",
    "Then when  $u = t$, $m = t+1$. $\\newline$\n",
    "And when $u = T-1$, $m = T$.\n",
    "So we get: \n",
    "\n",
    "$$\\sum_{u=t}^{T-1} \\gamma^{u - t + 1} V(S_{u+1})\n",
    "=\n",
    "\\sum_{m=t+1}^{T} \\gamma^{m - t} \\, V(S_m)$$.\n",
    "\n",
    "Similarly, keep  $\\sum_{u=t}^{T-1} \\gamma^{u - t} ,V(S_u)$\n",
    "as is (or rename index). \n",
    "\n",
    "Step 3. We notice that when we combine the  $V$  terms, we can say that\n",
    "\n",
    "$$\\sum_{m=t+1}^{T} \\gamma^{m - t}\\,V(S_m)\n",
    "\\;-\\;\n",
    "\\sum_{n=t}^{T-1} \\gamma^{n - t}\\,V(S_n)$$\n",
    "$$=\n",
    "-\\,\\gamma^{0}\\,V(S_t) \n",
    "\\;+\\;\\gamma^{T - t}\\,V(S_T)$$\n",
    "because all the intermediate terms cancel out in a telescoping sum.\n",
    "\n",
    "Hence, the entire expression becomes:\n",
    "\n",
    "$$\\sum_{u=t}^{T-1} \\gamma^{u - t} R_{u+1} \n",
    "\\;-\\; V(S_t)\n",
    "\\;+\\;\\gamma^{\\,T - t}\\,V(S_T).$$\n",
    "\n",
    "Step 4. Consider the terminal state assumption:\n",
    "\n",
    "If $S_T$ is terminal, we typically define  $V(S_T)=0$.\n",
    "Thus, \n",
    "$\\gamma^{T-t}\\,V(S_T) = 0$.\n",
    "Then:\n",
    "\n",
    "$$\\sum_{u=t}^{T-1} \\gamma^{u - t}\n",
    "\\bigl[R_{u+1} + \\gamma\\,V(S_{u+1}) - V(S_u)\\bigr]\n",
    "=\n",
    "\\sum_{u=t}^{T-1} \\gamma^{u-t}\\,R_{u+1} \n",
    "\\;-\\; V(S_t).$$\n",
    "\n",
    "\n",
    "But $\\sum_{u=t}^{T-1} \\gamma^{u-t}\\,R_{u+1}$\n",
    "is exactly $G_t$.\n",
    "Hence we get:\n",
    "\n",
    "$$G_t - V(S_t)\n",
    "=\n",
    "\\sum_{u=t}^{T-1}\\gamma^{u-t}\\,\n",
    "\\bigl[R_{u+1} + \\gamma\\,V(S_{u+1}) - V(S_u)\\bigr]$$ \n",
    "\n",
    "which completes the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All states in this 2D grid: [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "\n",
      "State (0, 0):\n",
      "  Terminal? True\n",
      "   (No transitions; this is a terminal state)\n",
      "\n",
      "State (0, 1):\n",
      "  Terminal? True\n",
      "   (No transitions; this is a terminal state)\n",
      "\n",
      "State (0, 2):\n",
      "  Terminal? True\n",
      "   (No transitions; this is a terminal state)\n",
      "\n",
      "State (1, 0):\n",
      "  Terminal? True\n",
      "   (No transitions; this is a terminal state)\n",
      "\n",
      "State (1, 1):\n",
      "  Terminal? False\n",
      "   -> Next: (1, 2), prob=0.25, reward=1.0\n",
      "   -> Next: (1, 0), prob=0.25, reward=0.0\n",
      "   -> Next: (0, 1), prob=0.25, reward=0.0\n",
      "   -> Next: (2, 1), prob=0.25, reward=1.0\n",
      "\n",
      "State (1, 2):\n",
      "  Terminal? True\n",
      "   (No transitions; this is a terminal state)\n",
      "\n",
      "State (2, 0):\n",
      "  Terminal? True\n",
      "   (No transitions; this is a terminal state)\n",
      "\n",
      "State (2, 1):\n",
      "  Terminal? True\n",
      "   (No transitions; this is a terminal state)\n",
      "\n",
      "State (2, 2):\n",
      "  Terminal? True\n",
      "   (No transitions; this is a terminal state)\n",
      "\n",
      "Simulating a few random episodes:\n",
      "Episode 0 starting at (1, 1): [((1, 1), 0.0), ((0, 1), 0.0)]\n",
      "Episode 1 starting at (1, 1): [((1, 1), 1.0), ((2, 1), 0.0)]\n",
      "Episode 2 starting at (1, 1): [((1, 1), 1.0), ((1, 2), 0.0)]\n"
     ]
    }
   ],
   "source": [
    "# random_walk_mrp_2d.py\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "import numpy as np\n",
    "\n",
    "class RandomWalkMRP2D:\n",
    "    \"\"\"\n",
    "    A 2D Random Walk Markov Reward Process. \n",
    "    - The state space is: (i, j) where 0 <= i <= B1, 0 <= j <= B2.\n",
    "    - Terminal states: all states on the boundary: i in {0, B1} or j in {0, B2}.\n",
    "    - Rewards: 0 for (0, j) or (i, 0), 1 for (B1, j) or (i, B2), 0 otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, B1: int, B2: int, gamma: float = 1.0):\n",
    "        \"\"\"\n",
    "        :param B1: Max boundary in x dimension\n",
    "        :param B2: Max boundary in y dimension\n",
    "        :param gamma: discount factor\n",
    "        \"\"\"\n",
    "        self.B1 = B1\n",
    "        self.B2 = B2\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def get_transition_probs_rewards(self, state: Tuple[int, int]) -> Dict[Tuple[int,int], Tuple[float, float]]:\n",
    "        \"\"\"\n",
    "        Return a dictionary: next_state -> (prob, reward).\n",
    "        If 'state' is terminal, return an empty dict.\n",
    "        \"\"\"\n",
    "        i, j = state\n",
    "\n",
    "        # Check if terminal\n",
    "        if i == 0 or i == self.B1 or j == 0 or j == self.B2:\n",
    "            return {}  # terminal, no transitions\n",
    "\n",
    "        transitions = {}\n",
    "        # Possible moves from (i,j): up=(i,j+1), down=(i,j-1),\n",
    "        # left=(i-1,j), right=(i+1,j).\n",
    "        actions = [(i, j+1), (i, j-1), (i-1, j), (i+1, j)]\n",
    "        prob = 0.25  # 4 equiprobable directions\n",
    "\n",
    "        for (nx, ny) in actions:\n",
    "            # Compute reward\n",
    "            if nx == 0 or ny == 0:\n",
    "                rew = 0.0\n",
    "            elif nx == self.B1 or ny == self.B2:\n",
    "                rew = 1.0\n",
    "            else:\n",
    "                rew = 0.0\n",
    "            \n",
    "            transitions[(nx, ny)] = (prob, rew)\n",
    "\n",
    "        return transitions\n",
    "\n",
    "    def is_terminal(self, state: Tuple[int,int]) -> bool:\n",
    "        i, j = state\n",
    "        # Return True if (i,j) is a boundary\n",
    "        return (i == 0 or i == self.B1 or j == 0 or j == self.B2)\n",
    "\n",
    "    def states(self):\n",
    "        \"\"\"\n",
    "        Yield all states in the 2D grid.\n",
    "        \"\"\"\n",
    "        for i in range(self.B1 + 1):\n",
    "            for j in range(self.B2 + 1):\n",
    "                yield (i, j)\n",
    "\n",
    "# To test functionality\n",
    "def main_test_random_walk_mrp_2d():\n",
    "    \"\"\"\n",
    "    Demonstration code for RandomWalkMRP2D:\n",
    "      1. Create a small 2D random walk MRP (e.g., 3x3).\n",
    "      2. Print all states and check which are terminal.\n",
    "      3. Print the transition probabilities/rewards for each state.\n",
    "      4. Simulate a few episodes from random starts.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Create an instance of the 2D MRP\n",
    "    B1, B2 = 2, 2  # small grid for demonstration\n",
    "    mrp_2d = RandomWalkMRP2D(B1, B2, gamma=1.0)\n",
    "    \n",
    "    # 2) List all states:\n",
    "    all_states = list(mrp_2d.states())\n",
    "    print(\"All states in this 2D grid:\", all_states)\n",
    "    \n",
    "    # 3) Show which states are terminal and the transitions from each state\n",
    "    for s in all_states:\n",
    "        print(f\"\\nState {s}:\")\n",
    "        print(\"  Terminal?\", mrp_2d.is_terminal(s))\n",
    "        transitions_dict = mrp_2d.get_transition_probs_rewards(s)\n",
    "        if transitions_dict:\n",
    "            for nxt, (prob, rew) in transitions_dict.items():\n",
    "                print(f\"   -> Next: {nxt}, prob={prob}, reward={rew}\")\n",
    "        else:\n",
    "            print(\"   (No transitions; this is a terminal state)\")\n",
    "    \n",
    "    #    For testing, simulate a few short episodes from random start states\n",
    "    #    Define a small helper function to step once, returning next_state,reward.\n",
    "    \n",
    "    def sample_next_state(mrp, state):\n",
    "        \"\"\"Given a state, sample next state & reward according to the MRP transitions.\"\"\"\n",
    "        trans_dict = mrp.get_transition_probs_rewards(state)\n",
    "        if not trans_dict:\n",
    "            # Terminal state, remain here with 0 reward\n",
    "            return state, 0.0\n",
    "        # Otherwise sample from the dict\n",
    "        next_states = list(trans_dict.keys())\n",
    "        probs = [trans_dict[sn][0] for sn in next_states]\n",
    "        rewards = [trans_dict[sn][1] for sn in next_states]\n",
    "        idx = np.random.choice(len(next_states), p=probs)\n",
    "        return next_states[idx], rewards[idx]\n",
    "    \n",
    "    num_episodes = 3\n",
    "    max_steps = 10\n",
    "    \n",
    "    print(\"\\nSimulating a few random episodes:\")\n",
    "    for e in range(num_episodes):\n",
    "        # Pick a random interior start state (not terminal)\n",
    "        candidates = [s for s in all_states if not mrp_2d.is_terminal(s)]\n",
    "        if not candidates:\n",
    "            print(\"No non-terminal states exist; entire grid is terminal!\")\n",
    "            break\n",
    "        \n",
    "        start_state = candidates[np.random.randint(len(candidates))]\n",
    "        episode = []\n",
    "        state = start_state\n",
    "        for step in range(max_steps):\n",
    "            nxt_state, r = sample_next_state(mrp_2d, state)\n",
    "            episode.append((state, r))\n",
    "            if mrp_2d.is_terminal(nxt_state):\n",
    "                episode.append((nxt_state, 0.0))\n",
    "                break\n",
    "            state = nxt_state\n",
    "        \n",
    "        print(f\"Episode {e} starting at {start_state}: {episode}\")\n",
    "\n",
    "\n",
    "# Finally, call the test function\n",
    "if __name__ == \"__main__\":\n",
    "    main_test_random_walk_mrp_2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7: Double Q-Learning (Led By: _____)\n",
    "\n",
    "It is known that **Q-Learning** can suffer from a maximization bias during finite-sample training. In this problem, we consider the following modification to the **Tabular Q-Learning** algorithm called **Double Q-Learning**:\n",
    "\n",
    "---\n",
    "\n",
    "### **Algorithm 1: Double Q-Learning**\n",
    "\n",
    "**Initialize** $Q_1(s, a)$ and $Q_2(s, a)$ $\\forall s \\in \\mathcal{N}$, $a \\in \\mathcal{A}$  \n",
    "**yield** estimate of $Q$\n",
    "\n",
    "**while** True **do**  \n",
    "&emsp; **select** initial state $s_0$, **set** $t = 0$  \n",
    "&emsp; **while** $s_t \\in \\mathcal{N}$ **do**  \n",
    "&emsp;&emsp; **select** $a_t$ using $\\epsilon$-greedy based on this greedy policy: $\\pi(s) = \\arg \\max_a \\big( Q_1(s_t, a) + Q_2(s_t, a) \\big)$  \n",
    "&emsp;&emsp; **observe** $(r_t, s_{t+1})$  \n",
    "&emsp;&emsp; **if** with 0.5 probability **then**  \n",
    "&emsp;&emsp;&emsp; $Q_1(s_t, a_t) \\leftarrow Q_1(s_t, a_t) + \\alpha \\big( r_t + \\gamma Q_2(s_{t+1}, \\arg \\max_a Q_2(s_{t+1}, a)) - Q_1(s_t, a_t) \\big)$  \n",
    "\n",
    "&emsp;&emsp; **else**  \n",
    "&emsp;&emsp;&emsp; $Q_2(s_t, a_t) \\leftarrow Q_2(s_t, a_t) + \\alpha \\big( r_t + \\gamma Q_1(s_{t+1}, \\arg \\max_a Q_1(s_{t+1}, a)) - Q_2(s_t, a_t) \\big)$  \n",
    "\n",
    "&emsp;&emsp; $t = t + 1$  \n",
    "&emsp;&emsp; $s_t = s_{t+1}$  \n",
    "\n",
    "**yield** estimate of $Q$\n",
    "\n",
    "---\n",
    "\n",
    "### **Algorithm 2: Q-Learning**\n",
    "\n",
    "**Initialize** $Q(s, a)$ $\\forall s \\in \\mathcal{N}$, $a \\in \\mathcal{A}$  \n",
    "**yield** $Q$\n",
    "\n",
    "**while** True **do**  \n",
    "&emsp; **select** initial state $s_0$, **set** $t = 0$  \n",
    "&emsp; **while** $s_t \\in \\mathcal{N}$ **do**  \n",
    "&emsp;&emsp; **select** $a_t$ using $\\epsilon$-greedy based on this greedy policy: $\\pi(s) = \\arg \\max_a Q(s_t, a)$  \n",
    "&emsp;&emsp; **observe** $(r_t, s_{t+1})$  \n",
    "&emsp;&emsp; $Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\big( r_t + \\gamma Q(s_{t+1}, \\arg \\max_a Q(s_{t+1}, a)) - Q(s_t, a_t) \\big)$  \n",
    "&emsp;&emsp; $t = t + 1$  \n",
    "&emsp;&emsp; $s_t = s_{t+1}$  \n",
    "\n",
    "**yield** $Q$\n",
    "\n",
    "---\n",
    "\n",
    "### Instructions:\n",
    "The code skeleton for this problem is provided below. Implement the following functions: `double_q_learning` and `q_learning`. Once implemented, you can run the code. You will get a graph of the estimated q-value plotted against the episode number. Comment on your observations, and explain the benefits/drawbacks of the double q-learning algorithm for general MDPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from typing import Mapping, Iterator, TypeVar, Tuple, Dict, Iterable, Generic\n",
    "\n",
    "# RL imports (adapt or remove if you don't have the same environment):\n",
    "from rl.distribution import (\n",
    "    Distribution, Constant, Gaussian, Choose, SampledDistribution, Categorical\n",
    ")\n",
    "from rl.markov_process import NonTerminal, State, Terminal\n",
    "from rl.markov_decision_process import MarkovDecisionProcess\n",
    "from rl.td import epsilon_greedy_action\n",
    "\n",
    "S = TypeVar('S')\n",
    "A = TypeVar('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# Tabular Q-value function approximation (done for you)\n",
    "# -----------------------------------------------------------------------\n",
    "class TabularQValueFunctionApprox(Generic[S, A]):\n",
    "    \"\"\"\n",
    "    A basic implementation of a tabular function approximation \n",
    "    with constant learning rate of 0.1\n",
    "    Also tracks the number of updates per (state, action).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.counts: Mapping[Tuple[NonTerminal[S], A], int] = defaultdict(int)\n",
    "        self.values: Mapping[Tuple[NonTerminal[S], A], float] = defaultdict(float)\n",
    "    \n",
    "    def update(self, k: Tuple[NonTerminal[S], A], target: float) -> None:\n",
    "        alpha = 0.1\n",
    "        old_val = self.values[k]\n",
    "        self.values[k] = (1 - alpha) * old_val + alpha * target\n",
    "        self.counts[k] += 1\n",
    "    \n",
    "    def __call__(self, x: Tuple[NonTerminal[S], A]) -> float:\n",
    "        return self.values[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# Double Q-Learning (fill in)\n",
    "# -----------------------------------------------------------------------\n",
    "def double_q_learning(\n",
    "    mdp: MarkovDecisionProcess[S, A],\n",
    "    states: Distribution[NonTerminal[S]],\n",
    "    gamma: float\n",
    ") -> Iterator[TabularQValueFunctionApprox[S, A]]:\n",
    "    \"\"\"\n",
    "    Implements Double Q-Learning as described:\n",
    "      1) We keep two Q-tables, Q1 and Q2.\n",
    "      2) We choose actions epsilon-greedily with respect to Q1+Q2.\n",
    "      3) With 50% chance we update Q1 using next-action chosen by max of Q2,\n",
    "         otherwise update Q2 using next-action chosen by max of Q1.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# Standard Q-Learning (fill in)\n",
    "# -----------------------------------------------------------------------\n",
    "def q_learning(\n",
    "    mdp: MarkovDecisionProcess[S, A],\n",
    "    states: Distribution[NonTerminal[S]],\n",
    "    gamma: float\n",
    ") -> Iterator[TabularQValueFunctionApprox[S, A]]:\n",
    "    \"\"\"\n",
    "    Standard Q-Learning:\n",
    "      1) Keep one Q table\n",
    "      2) Epsilon-greedy wrt that table\n",
    "      3) Update Q((s,a)) with  r + gamma * max_{a'} Q((s_next, a'))\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 115\u001b[0m\n\u001b[1;32m    112\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 115\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 93\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m all_std \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_RUNS):\n\u001b[0;32m---> 93\u001b[0m     dbl_vals \u001b[38;5;241m=\u001b[39m \u001b[43mrun_double_q_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmdp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_EPISODES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     std_vals \u001b[38;5;241m=\u001b[39m run_q_once(mdp, start_dist, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, episodes\u001b[38;5;241m=\u001b[39mN_EPISODES)\n\u001b[1;32m     95\u001b[0m     all_dbl\u001b[38;5;241m.\u001b[39mappend(dbl_vals)\n",
      "Cell \u001b[0;32mIn[45], line 58\u001b[0m, in \u001b[0;36mrun_double_q_once\u001b[0;34m(mdp, start_dist, gamma, episodes)\u001b[0m\n\u001b[1;32m     56\u001b[0m vals \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n\u001b[0;32m---> 58\u001b[0m     Q1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdq_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# record Q1((A,a1)) each time\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     qA1 \u001b[38;5;241m=\u001b[39m Q1((NonTerminal(P1State(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma1\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not an iterator"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# The MDP: States A,B and actions a1,a2,b1,...,bn (don't modify anything anymore, just run to get the graphs)\n",
    "# -----------------------------------------------------------------------\n",
    "@dataclass(frozen=True)\n",
    "class P1State:\n",
    "    \"\"\"\n",
    "    The MDP state, storing whether we are in \"A\" or \"B\".\n",
    "    \"\"\"\n",
    "    name: str\n",
    "\n",
    "class P1MDP(MarkovDecisionProcess[P1State, str]):\n",
    "    \n",
    "    def __init__(self, n: int):\n",
    "        self.n = n\n",
    "\n",
    "    def actions(self, state: NonTerminal[P1State]) -> Iterable[str]:\n",
    "        \"\"\"\n",
    "        Return the actions available from this state.\n",
    "          - if state is A => [\"a1\", \"a2\"]\n",
    "          - if state is B => [\"b1\", ..., \"bn\"]\n",
    "        \"\"\"\n",
    "        if state.state.name == \"A\":\n",
    "            return [\"a1\", \"a2\"]\n",
    "        else:\n",
    "            return [f\"b{i}\" for i in range(1, self.n+1)]\n",
    "    \n",
    "    def step(\n",
    "        self,\n",
    "        state: NonTerminal[P1State],\n",
    "        action: str\n",
    "    ) -> Distribution[Tuple[State[P1State], float]]:\n",
    "        \"\"\"\n",
    "        Return the distribution of (next state, reward) from (state, action):\n",
    "          - A + a1 => reward 0, next state B\n",
    "          - A + a2 => reward 0, next state terminal\n",
    "          - B + b_i => reward ~ Normal(-0.1,1), next state terminal\n",
    "        \"\"\"\n",
    "        if state.state.name == \"A\":\n",
    "            if action == \"a1\":\n",
    "                return Constant((NonTerminal(P1State(\"B\")), 0.0))\n",
    "            else:\n",
    "                return Constant((Terminal(P1State(\"T\")), 0.0))\n",
    "        else:\n",
    "            # For B + b_i => reward ~ N(-0.1,1), then terminal\n",
    "            def sampler():\n",
    "                r = np.random.normal(loc=-0.1, scale=1.0)\n",
    "                return (Terminal(P1State(\"T\")), r)\n",
    "            return SampledDistribution(sampler)\n",
    "\n",
    "def run_double_q_once(mdp, start_dist, gamma=1.0, episodes=400):\n",
    "    \"\"\"\n",
    "    Runs one 'chain' of Double Q-Learning for 'episodes' episodes,\n",
    "    returning a list of Q-values for Q((A,a1)) at the end of each episode.\n",
    "    \"\"\"\n",
    "    dq_iter = double_q_learning(mdp, start_dist, gamma)  # generator\n",
    "    vals = []\n",
    "    for _ in range(episodes):\n",
    "        Q1 = next(dq_iter)\n",
    "        # record Q1((A,a1)) each time\n",
    "        qA1 = Q1((NonTerminal(P1State(\"A\")), \"a1\"))\n",
    "        vals.append(qA1)\n",
    "    return vals\n",
    "\n",
    "def run_q_once(mdp, start_dist, gamma=1.0, episodes=400):\n",
    "    \"\"\"\n",
    "    Same but for standard Q-Learning\n",
    "    \"\"\"\n",
    "    q_iter = q_learning(mdp, start_dist, gamma)\n",
    "    vals = []\n",
    "    for _ in range(episodes):\n",
    "        Q = next(q_iter)\n",
    "        qA1 = Q((NonTerminal(P1State(\"A\")), \"a1\"))\n",
    "        vals.append(qA1)\n",
    "    return vals\n",
    "\n",
    "def main():\n",
    "    # For reproducibility\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    n = 10\n",
    "    mdp = P1MDP(n)\n",
    "    # Always start in A, as a NonTerminal\n",
    "    start_dist = Constant(NonTerminal(P1State(\"A\")))\n",
    "\n",
    "    N_RUNS = 100\n",
    "    N_EPISODES = 400\n",
    "\n",
    "    all_dbl = []\n",
    "    all_std = []\n",
    "\n",
    "    for _ in range(N_RUNS):\n",
    "        dbl_vals = run_double_q_once(mdp, start_dist, gamma=1.0, episodes=N_EPISODES)\n",
    "        std_vals = run_q_once(mdp, start_dist, gamma=1.0, episodes=N_EPISODES)\n",
    "        all_dbl.append(dbl_vals)\n",
    "        all_std.append(std_vals)\n",
    "\n",
    "    arr_dbl = np.array(all_dbl)\n",
    "    arr_std = np.array(all_std)\n",
    "\n",
    "    avg_dbl = np.mean(arr_dbl, axis=0)\n",
    "    avg_std = np.mean(arr_std, axis=0)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(avg_dbl, label='Double Q-Learning: Q(A,a1)')\n",
    "    plt.plot(avg_std, label='Q-Learning: Q(A,a1)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Estimated Q-value')\n",
    "    plt.title('Average Q(A,a1) over 100 runs, n=10')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8: Dice Rolling Game (Led By: _____)\n",
    "\n",
    "Consider the following dice game. You start with $N$ $K$-sided dice on the table, and no dice in your hand. The values on the dice faces are $\\{1, 2, ..., K\\}$. While you have dice remaining on the table, the game proceeds as follows:\n",
    "\n",
    "1. **Roll all the dice on the table.**  \n",
    "2. **Select a nonempty subset of the dice on the table to move to your hand.**  \n",
    "   - The dice you move to your hand keep the value which they were just rolled.  \n",
    "   - For example, if your hand is $\\{1, 3\\}$ and you roll $\\{2, 2, 3, 4\\}$ on the table, and you decide to move the dice with $3$ and $4$ to your hand, you will now have $\\{1, 3, 3, 4\\}$ in your hand.\n",
    "\n",
    "The game ends when you have no dice on the table left to roll. Your score for the game is then calculated as the sum of the values of dice in your hand **if you have at least $C$ 1’s in your hand**, and zero otherwise. For example:\n",
    "- For $N = K = 4$ and $C = 2$, the score corresponding to a hand containing $\\{1, 3, 1, 4\\}$ would be $9$, while the score corresponding to a hand containing $\\{4, 1, 3, 4\\}$ would be $0$.\n",
    "\n",
    "Your goal is to **maximize your score** at the end of the game.\n",
    "\n",
    "---\n",
    "\n",
    "### Part (A): MDP Modeling\n",
    "\n",
    "With proper mathematical notation, model this as a **Finite MDP** specifying the following:\n",
    "- **States**\n",
    "- **Actions**\n",
    "- **Rewards**\n",
    "- **State-Transition Probabilities**\n",
    "- **Discount Factor**\n",
    "\n",
    "---\n",
    "\n",
    "### Part (B): Python Implementation\n",
    "\n",
    "Implement this MDP in Python. If you wish, you may use the code in the git repo that you forked at the start of the course (e.g., `FiniteMarkovDecisionProcess`), but if you prefer, you can implement it from scratch or use any code you have written for the course previously (whichever is more convenient for you). You should implement this for the **general case**, specifically your MDP implementation should take as parameters $N$, $K$, $C$.\n",
    "\n",
    "For $N = 6$, $K = 4$, $C = 1$, use the `dynamic_programming.py` library (or your own code if you chose not to implement it within the class library) to solve for the **optimal value function**, and present the following values:\n",
    "\n",
    "1. The **expected score** of the game playing optimally, calculated using your code, not analytically.\n",
    "2. The **optimal action** when rolling $\\{1, 2, 2, 3, 3, 4\\}$ on the first roll."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "<span style=\"color:red\">*fill in*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in with Python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected Score:\n",
    "\n",
    "Optimal Action:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cme241",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
